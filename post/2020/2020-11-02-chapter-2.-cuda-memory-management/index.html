<!doctype html><html lang=en-us><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Chapter 2. CUDA Memory Management | Keep calm and Write something</title><meta name=title content="Chapter 2. CUDA Memory Management"><meta name=description content="2. CUDA Memory Management

Most of the application’s performance will be bottlecked by memory-related constraints
GPU RAM BW : 900GB/s (DDR3 ?)
NV Visual Profiler
Global memory is a staging area where all of the date gets copied from CPU memory.
Global Memory(device memory) is visible to all of the threads in the kernel and also visible to CPu.
Coalesced vs. uncoalesced global memory access

coalesced global memory access : Sequential memory access is adjacent



Warp

Warp is a unit of thread scheduling/execution in SMs. Once a block has been assigned to an SM, it is divided into a 32-threads unit known as a warp
Among all of the available warps, the ones with operands that are ready for the next instruction become eligible for execution.
All of the threads in a warp execute the same instruction when selected.

AOS vs. SOA

AOS : Array of Structure. A[0].a, A[1].a, ...
SOA : Structure of Array. Each member of structure is array S.a[0], S[1], ...

Suitable for SIMT - same operation for the same member with different array index. In this case, the threads of the same block access adjacent memory spaces in turn increase the spatial locality.


As a GPU is latency-hiding architecture, it becomes important to saturate the memory bandwidth.

Shared Memory

User-Managed Cache
Shared memory is only visible to the threads in the same block.

All of the threads in a block see the same version of shared memory.
Threads in other SM can not see this shared memory
Another block has its own shared memory
Even in the same SM, different block has different share memory.


Key usage of shared memory comes from that threads within a block can share memory access

Shared variable can be located in the shared memory to be accessed multiple times.
CDUA 9.0 provides inter-thread communication between ones in the different SMs


Bank

Shared memory is organized into banks to achieve higher bandwidth.
Each bank can serve one address per cycle.
Volta GPU has 32 banks each 4 bytes wide. 128 Bytes at one cycle
Bank Conflict

If multiple threads access the same Bank, the access to the shared memory is serialized. This should be avoid if possible.





Read-only Cache

Referred to as texture cache
const __restrict__
Ideally used when the entire warp to read the same address/data

Registers

Scope : a single thread. Each thread has its own registers
Local variables are stored in the registers

Too many local variable can cause performance issue as the data should be reside in L1 or L2 cache or device memory
register spills



Pinned-memory

Recommendations to reduce host/device memory copy

Minimize amount of data to be transferred
Use the pinned memory
Batch small transfers into one large transfer
Asynchronous data transfer


malloc allocates pageable memory

Device including GPU can not access the pageable memory.
When the device access the pageable memory, the driver allocate temporary pinned memory and copy the data from the pageable memory  and do DMA
This introduce additional latency


cudaMallocHost allocates pinned memory from the system memory.

Too much use of pinned memory impact on the system performance as non-pageable memory is also used by the system(OS)



Unified Memory

Provide a single memory space accessible from CPU and GPU - easy programming and porting the CPU application to GPU
Allow over-subscription
A single pointer is used by the CPU and GPU while non-unified memory case, each has to have its own pointer as host memory and device memory are different.
cudaMallocManaged does not allocate the physical memory when it is called but it allocate when the data is touch for the first time. This requires page migration and introduce additional time.

Workaround 1 : define an initialization kernel on the GPU which touch the unified memory space on behalf of the workload kernel.
Workaround 2: Prefetch




Initialization kernel

왜 page fault 횟수가 줄어드는 지 잘 이해가 안되네&mldr;. 그리고 여러 thread가 동시에 접근하는 건 page fault 회수가 1번인가?? 예제가 잘 이해가 안됨&mldr;

Prefetch


cudaMemPrefetchAsync"><meta name=keywords content="CUDA,“GPU”,“book”,"><meta property="og:url" content="https://cychong47.github.io/post/2020/2020-11-02-chapter-2.-cuda-memory-management/"><meta property="og:site_name" content="Keep calm and Write something"><meta property="og:title" content="Chapter 2. CUDA Memory Management"><meta property="og:description" content="2. CUDA Memory Management Most of the application’s performance will be bottlecked by memory-related constraints GPU RAM BW : 900GB/s (DDR3 ?) NV Visual Profiler Global memory is a staging area where all of the date gets copied from CPU memory. Global Memory(device memory) is visible to all of the threads in the kernel and also visible to CPu. Coalesced vs. uncoalesced global memory access coalesced global memory access : Sequential memory access is adjacent Warp Warp is a unit of thread scheduling/execution in SMs. Once a block has been assigned to an SM, it is divided into a 32-threads unit known as a warp Among all of the available warps, the ones with operands that are ready for the next instruction become eligible for execution. All of the threads in a warp execute the same instruction when selected. AOS vs. SOA AOS : Array of Structure. A[0].a, A[1].a, ... SOA : Structure of Array. Each member of structure is array S.a[0], S[1], ... Suitable for SIMT - same operation for the same member with different array index. In this case, the threads of the same block access adjacent memory spaces in turn increase the spatial locality. As a GPU is latency-hiding architecture, it becomes important to saturate the memory bandwidth. Shared Memory User-Managed Cache Shared memory is only visible to the threads in the same block. All of the threads in a block see the same version of shared memory. Threads in other SM can not see this shared memory Another block has its own shared memory Even in the same SM, different block has different share memory. Key usage of shared memory comes from that threads within a block can share memory access Shared variable can be located in the shared memory to be accessed multiple times. CDUA 9.0 provides inter-thread communication between ones in the different SMs Bank Shared memory is organized into banks to achieve higher bandwidth. Each bank can serve one address per cycle. Volta GPU has 32 banks each 4 bytes wide. 128 Bytes at one cycle Bank Conflict If multiple threads access the same Bank, the access to the shared memory is serialized. This should be avoid if possible. Read-only Cache Referred to as texture cache const __restrict__ Ideally used when the entire warp to read the same address/data Registers Scope : a single thread. Each thread has its own registers Local variables are stored in the registers Too many local variable can cause performance issue as the data should be reside in L1 or L2 cache or device memory register spills Pinned-memory Recommendations to reduce host/device memory copy Minimize amount of data to be transferred Use the pinned memory Batch small transfers into one large transfer Asynchronous data transfer malloc allocates pageable memory Device including GPU can not access the pageable memory. When the device access the pageable memory, the driver allocate temporary pinned memory and copy the data from the pageable memory and do DMA This introduce additional latency cudaMallocHost allocates pinned memory from the system memory. Too much use of pinned memory impact on the system performance as non-pageable memory is also used by the system(OS) Unified Memory Provide a single memory space accessible from CPU and GPU - easy programming and porting the CPU application to GPU Allow over-subscription A single pointer is used by the CPU and GPU while non-unified memory case, each has to have its own pointer as host memory and device memory are different. cudaMallocManaged does not allocate the physical memory when it is called but it allocate when the data is touch for the first time. This requires page migration and introduce additional time. Workaround 1 : define an initialization kernel on the GPU which touch the unified memory space on behalf of the workload kernel. Workaround 2: Prefetch Initialization kernel 왜 page fault 횟수가 줄어드는 지 잘 이해가 안되네…. 그리고 여러 thread가 동시에 접근하는 건 page fault 회수가 1번인가?? 예제가 잘 이해가 안됨… Prefetch cudaMemPrefetchAsync"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-11-02T08:56:16+09:00"><meta property="article:modified_time" content="2020-11-02T08:56:16+09:00"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="“GPU”"><meta property="article:tag" content="“Book”"><meta property="og:see_also" content="https://cychong47.github.io/post/2020/2020-11-02-chapter-1-introduction-to-cuda-programming/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Chapter 2. CUDA Memory Management"><meta name=twitter:description content="2. CUDA Memory Management Most of the application’s performance will be bottlecked by memory-related constraints GPU RAM BW : 900GB/s (DDR3 ?) NV Visual Profiler Global memory is a staging area where all of the date gets copied from CPU memory. Global Memory(device memory) is visible to all of the threads in the kernel and also visible to CPu. Coalesced vs. uncoalesced global memory access coalesced global memory access : Sequential memory access is adjacent Warp Warp is a unit of thread scheduling/execution in SMs. Once a block has been assigned to an SM, it is divided into a 32-threads unit known as a warp Among all of the available warps, the ones with operands that are ready for the next instruction become eligible for execution. All of the threads in a warp execute the same instruction when selected. AOS vs. SOA AOS : Array of Structure. A[0].a, A[1].a, ... SOA : Structure of Array. Each member of structure is array S.a[0], S[1], ... Suitable for SIMT - same operation for the same member with different array index. In this case, the threads of the same block access adjacent memory spaces in turn increase the spatial locality. As a GPU is latency-hiding architecture, it becomes important to saturate the memory bandwidth. Shared Memory User-Managed Cache Shared memory is only visible to the threads in the same block. All of the threads in a block see the same version of shared memory. Threads in other SM can not see this shared memory Another block has its own shared memory Even in the same SM, different block has different share memory. Key usage of shared memory comes from that threads within a block can share memory access Shared variable can be located in the shared memory to be accessed multiple times. CDUA 9.0 provides inter-thread communication between ones in the different SMs Bank Shared memory is organized into banks to achieve higher bandwidth. Each bank can serve one address per cycle. Volta GPU has 32 banks each 4 bytes wide. 128 Bytes at one cycle Bank Conflict If multiple threads access the same Bank, the access to the shared memory is serialized. This should be avoid if possible. Read-only Cache Referred to as texture cache const __restrict__ Ideally used when the entire warp to read the same address/data Registers Scope : a single thread. Each thread has its own registers Local variables are stored in the registers Too many local variable can cause performance issue as the data should be reside in L1 or L2 cache or device memory register spills Pinned-memory Recommendations to reduce host/device memory copy Minimize amount of data to be transferred Use the pinned memory Batch small transfers into one large transfer Asynchronous data transfer malloc allocates pageable memory Device including GPU can not access the pageable memory. When the device access the pageable memory, the driver allocate temporary pinned memory and copy the data from the pageable memory and do DMA This introduce additional latency cudaMallocHost allocates pinned memory from the system memory. Too much use of pinned memory impact on the system performance as non-pageable memory is also used by the system(OS) Unified Memory Provide a single memory space accessible from CPU and GPU - easy programming and porting the CPU application to GPU Allow over-subscription A single pointer is used by the CPU and GPU while non-unified memory case, each has to have its own pointer as host memory and device memory are different. cudaMallocManaged does not allocate the physical memory when it is called but it allocate when the data is touch for the first time. This requires page migration and introduce additional time. Workaround 1 : define an initialization kernel on the GPU which touch the unified memory space on behalf of the workload kernel. Workaround 2: Prefetch Initialization kernel 왜 page fault 횟수가 줄어드는 지 잘 이해가 안되네…. 그리고 여러 thread가 동시에 접근하는 건 page fault 회수가 1번인가?? 예제가 잘 이해가 안됨… Prefetch cudaMemPrefetchAsync"><meta itemprop=name content="Chapter 2. CUDA Memory Management"><meta itemprop=description content="2. CUDA Memory Management Most of the application’s performance will be bottlecked by memory-related constraints GPU RAM BW : 900GB/s (DDR3 ?) NV Visual Profiler Global memory is a staging area where all of the date gets copied from CPU memory. Global Memory(device memory) is visible to all of the threads in the kernel and also visible to CPu. Coalesced vs. uncoalesced global memory access coalesced global memory access : Sequential memory access is adjacent Warp Warp is a unit of thread scheduling/execution in SMs. Once a block has been assigned to an SM, it is divided into a 32-threads unit known as a warp Among all of the available warps, the ones with operands that are ready for the next instruction become eligible for execution. All of the threads in a warp execute the same instruction when selected. AOS vs. SOA AOS : Array of Structure. A[0].a, A[1].a, ... SOA : Structure of Array. Each member of structure is array S.a[0], S[1], ... Suitable for SIMT - same operation for the same member with different array index. In this case, the threads of the same block access adjacent memory spaces in turn increase the spatial locality. As a GPU is latency-hiding architecture, it becomes important to saturate the memory bandwidth. Shared Memory User-Managed Cache Shared memory is only visible to the threads in the same block. All of the threads in a block see the same version of shared memory. Threads in other SM can not see this shared memory Another block has its own shared memory Even in the same SM, different block has different share memory. Key usage of shared memory comes from that threads within a block can share memory access Shared variable can be located in the shared memory to be accessed multiple times. CDUA 9.0 provides inter-thread communication between ones in the different SMs Bank Shared memory is organized into banks to achieve higher bandwidth. Each bank can serve one address per cycle. Volta GPU has 32 banks each 4 bytes wide. 128 Bytes at one cycle Bank Conflict If multiple threads access the same Bank, the access to the shared memory is serialized. This should be avoid if possible. Read-only Cache Referred to as texture cache const __restrict__ Ideally used when the entire warp to read the same address/data Registers Scope : a single thread. Each thread has its own registers Local variables are stored in the registers Too many local variable can cause performance issue as the data should be reside in L1 or L2 cache or device memory register spills Pinned-memory Recommendations to reduce host/device memory copy Minimize amount of data to be transferred Use the pinned memory Batch small transfers into one large transfer Asynchronous data transfer malloc allocates pageable memory Device including GPU can not access the pageable memory. When the device access the pageable memory, the driver allocate temporary pinned memory and copy the data from the pageable memory and do DMA This introduce additional latency cudaMallocHost allocates pinned memory from the system memory. Too much use of pinned memory impact on the system performance as non-pageable memory is also used by the system(OS) Unified Memory Provide a single memory space accessible from CPU and GPU - easy programming and porting the CPU application to GPU Allow over-subscription A single pointer is used by the CPU and GPU while non-unified memory case, each has to have its own pointer as host memory and device memory are different. cudaMallocManaged does not allocate the physical memory when it is called but it allocate when the data is touch for the first time. This requires page migration and introduce additional time. Workaround 1 : define an initialization kernel on the GPU which touch the unified memory space on behalf of the workload kernel. Workaround 2: Prefetch Initialization kernel 왜 page fault 횟수가 줄어드는 지 잘 이해가 안되네…. 그리고 여러 thread가 동시에 접근하는 건 page fault 회수가 1번인가?? 예제가 잘 이해가 안됨… Prefetch cudaMemPrefetchAsync"><meta itemprop=datePublished content="2020-11-02T08:56:16+09:00"><meta itemprop=dateModified content="2020-11-02T08:56:16+09:00"><meta itemprop=wordCount content="685"><meta itemprop=keywords content="CUDA,“GPU”,“Book”"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:720px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fafafa;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#fafafa;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--blockquote-color:#222}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}time{font-family:monospace;font-style:normal;font-size:15px}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;border-radius:3px}blockquote{border-left:1px solid #999;color:var(--blockquote-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto!important}.highlight,.code{border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ class=title><h2>Keep calm and Write something</h2></a><nav><a href=/post/>Post</a>
<a href=/page/>Page</a>
<a href=/series/>Series</a>
<a href=/tags/>Tag</a>
<a href=/archive/>Archive</a>
<a href=/search/>Search</a></nav></header><main><content><h1 id=2-cuda-memory-management>2. CUDA Memory Management</h1><ul><li>Most of the application’s performance will be bottlecked by memory-related constraints</li><li>GPU RAM BW : 900GB/s (DDR3 ?)</li><li>NV Visual Profiler</li><li>Global memory is a <strong>staging area</strong> where all of the date gets copied from CPU memory.</li><li>Global Memory(device memory) is visible to all of the threads in the kernel and also visible to CPu.</li><li>Coalesced vs. uncoalesced global memory access<ul><li>coalesced global memory access : Sequential memory access is adjacent</li></ul></li></ul><h2 id=warp>Warp</h2><ul><li>Warp is a unit of thread scheduling/execution in SMs. Once a block has been assigned to an SM, it is divided into a 32-threads unit known as a <strong>warp</strong></li><li>Among all of the available warps, the ones with operands that are ready for the next instruction become <strong>eligible</strong> for execution.</li><li>All of the threads in a warp execute the same instruction when selected.</li></ul><h3 id=aos-vs-soa>AOS vs. SOA</h3><ul><li>AOS : Array of Structure. <code>A[0].a, A[1].a, ...</code></li><li>SOA : Structure of Array. Each member of structure is array <code>S.a[0], S[1], ...</code><ul><li>Suitable for SIMT - same operation for the same member with different array index. In this case, the threads of the same block access adjacent memory spaces in turn increase the spatial locality.</li></ul></li><li>As a GPU is latency-hiding architecture, it becomes important to saturate the memory bandwidth.</li></ul><h2 id=shared-memory>Shared Memory</h2><ul><li><strong>User-Managed Cache</strong></li><li>Shared memory is only visible to the threads <strong>in the same block</strong>.<ul><li>All of the threads in a block see the same version of shared memory.</li><li>Threads in other SM can not see this shared memory</li><li>Another block has its own shared memory</li><li>Even in the same SM, different block has different share memory.</li></ul></li><li>Key usage of shared memory comes from that threads within a block can share memory access<ul><li>Shared variable can be located in the shared memory to be accessed multiple times.</li><li>CDUA 9.0 provides inter-thread communication between ones in the different SMs</li></ul></li><li>Bank<ul><li>Shared memory is organized into banks to achieve higher bandwidth.</li><li>Each bank can serve one address per cycle.</li><li>Volta GPU has 32 banks each 4 bytes wide. 128 Bytes at one cycle</li><li>Bank Conflict<ul><li>If multiple threads access the same Bank, the access to the shared memory is serialized. This should be avoid if possible.</li></ul></li></ul></li></ul><h2 id=read-only-cache>Read-only Cache</h2><ul><li>Referred to as <strong>texture cache</strong></li><li><code>const __restrict__</code></li><li>Ideally used when the entire warp to read the same address/data</li></ul><h2 id=registers>Registers</h2><ul><li>Scope : a single thread. Each thread has its own registers</li><li>Local variables are stored in the registers<ul><li>Too many local variable can cause performance issue as the data should be reside in L1 or L2 cache or device memory</li><li><strong>register spills</strong></li></ul></li></ul><h2 id=pinned-memory>Pinned-memory</h2><ul><li>Recommendations to reduce host/device memory copy<ul><li>Minimize amount of data to be transferred</li><li>Use the <strong>pinned memory</strong></li><li>Batch small transfers into one large transfer</li><li>Asynchronous data transfer</li></ul></li><li><code>malloc</code> allocates pageable memory<ul><li>Device including GPU can not access the pageable memory.</li><li>When the device access the pageable memory, the driver allocate temporary pinned memory and copy the data from the pageable memory and do DMA</li><li>This introduce additional latency</li></ul></li><li><code>cudaMallocHost</code> allocates pinned memory from the system memory.<ul><li>Too much use of pinned memory impact on the system performance as non-pageable memory is also used by the system(OS)</li></ul></li></ul><h2 id=unified-memory>Unified Memory</h2><ul><li>Provide a single memory space accessible from CPU and GPU - easy programming and porting the CPU application to GPU</li><li>Allow over-subscription</li><li>A single pointer is used by the CPU and GPU while non-unified memory case, each has to have its own pointer as host memory and device memory are different.</li><li><code>cudaMallocManaged</code> does not allocate the physical memory when it is called but it allocate when the data is touch for the first time. This requires <code>page migration</code> and introduce additional time.<ul><li>Workaround 1 : define an initialization kernel on the GPU which <strong>touch</strong> the unified memory space on behalf of the workload kernel.</li><li>Workaround 2: Prefetch</li></ul></li><li></li></ul><h3 id=initialization-kernel>Initialization kernel</h3><ul><li>왜 page fault 횟수가 줄어드는 지 잘 이해가 안되네&mldr;. 그리고 여러 thread가 동시에 접근하는 건 page fault 회수가 1번인가?? 예제가 잘 이해가 안됨&mldr;</li></ul><h3 id=prefetch>Prefetch</h3><ul><li><p><code>cudaMemPrefetchAsync</code></p></li><li><p><code>cudaMemAdviseSetReadyMostly</code></p></li><li><p><code>cudaMemAdviseSetPreferredLocation</code></p></li><li><p><code>cudaMemAdviseSetAccessedBy</code></p></li><li><p>Volta combines the shared memory and L1 cache</p><ul><li>Total 128KB</li><li>Shared memory can be allocated up to 96KB</li></ul></li></ul><p>#cuda #book/learn-cuda-programming</p></content><p><a href=https://cychong47.github.io/tags/cuda/>#CUDA</a>
<a href=https://cychong47.github.io/tags/gpu/>#“GPU”</a>
<a href=https://cychong47.github.io/tags/book/>#“Book”</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>