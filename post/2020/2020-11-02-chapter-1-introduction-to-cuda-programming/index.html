<!doctype html><html lang=en-us><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Chapter 1. Introduction to CUDA Programming | Keep calm and Write something</title><meta name=title content="Chapter 1. Introduction to CUDA Programming"><meta name=description content="1. Introduction to CUDA Programming

CPU Architecture is optimized for low latency accessing while GPU architecture is optimized for data parallel throughput compution
CPU hides latency of data by frequently stroring used data in caches and utilize the temporal locality
In CUDA, the execution unit is a warp not a thread. Context switching is happens between the warps and not threads.
GPU has lots of registers, all the thread context switching information is already present in the registers.(No context switching overhead unlike the CPU)
Host code vs. Device code.
Host memory vs. Device memory
The return type of device function is always void.
Data-parallel portions of an algorithm are executed on the device are kernels.
All the kernels in CUDA are asynchronous in nature. Host need to wait for the device to finish. cudaDeviceSynchronize
Software X runs on/as HW Y

CUDA thread <-> CUDA core/SIMD code
CUDA block <-> SM
Grid/kenrel <-> GPU device


One block runs on a single SM. All the threads within one block can only execute on cores in one SM.
<< BlockDim, ThreadDim >>

blockIdx, threadIdx : Index
blockDim, threadDim : Dimension (==Size)
blockDim is the number of threads per block


Threads have mechanism to communicate and synchronize efficiently.

The CUDA programming model allows this communication for threads whiten the same block
The therads communicate with each other in the same block using a special memory shared memory


Threads belonging to different block cannot communicate/synchronize with each other during the execution of the kernel.
cudaError_t e.  cudaGetLastError. Even for the multiple error, only the last one is returned. a<<< , >>>; cudaDeviceSynchronize(); e = cudaGetLastError();

Backlink
[[Learn CUDA Programming]]"><meta name=keywords content="CUDA,“GPU”,“book”,"><meta property="og:url" content="https://cychong47.github.io/post/2020/2020-11-02-chapter-1-introduction-to-cuda-programming/"><meta property="og:site_name" content="Keep calm and Write something"><meta property="og:title" content="Chapter 1. Introduction to CUDA Programming"><meta property="og:description" content="1. Introduction to CUDA Programming CPU Architecture is optimized for low latency accessing while GPU architecture is optimized for data parallel throughput compution CPU hides latency of data by frequently stroring used data in caches and utilize the temporal locality In CUDA, the execution unit is a warp not a thread. Context switching is happens between the warps and not threads. GPU has lots of registers, all the thread context switching information is already present in the registers.(No context switching overhead unlike the CPU) Host code vs. Device code. Host memory vs. Device memory The return type of device function is always void. Data-parallel portions of an algorithm are executed on the device are kernels. All the kernels in CUDA are asynchronous in nature. Host need to wait for the device to finish. cudaDeviceSynchronize Software X runs on/as HW Y CUDA thread <-> CUDA core/SIMD code CUDA block <-> SM Grid/kenrel <-> GPU device One block runs on a single SM. All the threads within one block can only execute on cores in one SM. << BlockDim, ThreadDim >> blockIdx, threadIdx : Index blockDim, threadDim : Dimension (==Size) blockDim is the number of threads per block Threads have mechanism to communicate and synchronize efficiently. The CUDA programming model allows this communication for threads whiten the same block The therads communicate with each other in the same block using a special memory shared memory Threads belonging to different block cannot communicate/synchronize with each other during the execution of the kernel. cudaError_t e. cudaGetLastError. Even for the multiple error, only the last one is returned. a<<< , >>>; cudaDeviceSynchronize(); e = cudaGetLastError(); Backlink [[Learn CUDA Programming]]"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-11-02T08:55:34+09:00"><meta property="article:modified_time" content="2020-11-02T08:55:34+09:00"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="“GPU”"><meta property="article:tag" content="“Book”"><meta property="og:see_also" content="https://cychong47.github.io/post/2020/2020-11-02-chapter-2.-cuda-memory-management/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Chapter 1. Introduction to CUDA Programming"><meta name=twitter:description content="1. Introduction to CUDA Programming CPU Architecture is optimized for low latency accessing while GPU architecture is optimized for data parallel throughput compution CPU hides latency of data by frequently stroring used data in caches and utilize the temporal locality In CUDA, the execution unit is a warp not a thread. Context switching is happens between the warps and not threads. GPU has lots of registers, all the thread context switching information is already present in the registers.(No context switching overhead unlike the CPU) Host code vs. Device code. Host memory vs. Device memory The return type of device function is always void. Data-parallel portions of an algorithm are executed on the device are kernels. All the kernels in CUDA are asynchronous in nature. Host need to wait for the device to finish. cudaDeviceSynchronize Software X runs on/as HW Y CUDA thread <-> CUDA core/SIMD code CUDA block <-> SM Grid/kenrel <-> GPU device One block runs on a single SM. All the threads within one block can only execute on cores in one SM. << BlockDim, ThreadDim >> blockIdx, threadIdx : Index blockDim, threadDim : Dimension (==Size) blockDim is the number of threads per block Threads have mechanism to communicate and synchronize efficiently. The CUDA programming model allows this communication for threads whiten the same block The therads communicate with each other in the same block using a special memory shared memory Threads belonging to different block cannot communicate/synchronize with each other during the execution of the kernel. cudaError_t e. cudaGetLastError. Even for the multiple error, only the last one is returned. a<<< , >>>; cudaDeviceSynchronize(); e = cudaGetLastError(); Backlink [[Learn CUDA Programming]]"><meta itemprop=name content="Chapter 1. Introduction to CUDA Programming"><meta itemprop=description content="1. Introduction to CUDA Programming CPU Architecture is optimized for low latency accessing while GPU architecture is optimized for data parallel throughput compution CPU hides latency of data by frequently stroring used data in caches and utilize the temporal locality In CUDA, the execution unit is a warp not a thread. Context switching is happens between the warps and not threads. GPU has lots of registers, all the thread context switching information is already present in the registers.(No context switching overhead unlike the CPU) Host code vs. Device code. Host memory vs. Device memory The return type of device function is always void. Data-parallel portions of an algorithm are executed on the device are kernels. All the kernels in CUDA are asynchronous in nature. Host need to wait for the device to finish. cudaDeviceSynchronize Software X runs on/as HW Y CUDA thread <-> CUDA core/SIMD code CUDA block <-> SM Grid/kenrel <-> GPU device One block runs on a single SM. All the threads within one block can only execute on cores in one SM. << BlockDim, ThreadDim >> blockIdx, threadIdx : Index blockDim, threadDim : Dimension (==Size) blockDim is the number of threads per block Threads have mechanism to communicate and synchronize efficiently. The CUDA programming model allows this communication for threads whiten the same block The therads communicate with each other in the same block using a special memory shared memory Threads belonging to different block cannot communicate/synchronize with each other during the execution of the kernel. cudaError_t e. cudaGetLastError. Even for the multiple error, only the last one is returned. a<<< , >>>; cudaDeviceSynchronize(); e = cudaGetLastError(); Backlink [[Learn CUDA Programming]]"><meta itemprop=datePublished content="2020-11-02T08:55:34+09:00"><meta itemprop=dateModified content="2020-11-02T08:55:34+09:00"><meta itemprop=wordCount content="282"><meta itemprop=keywords content="CUDA,“GPU”,“Book”"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:720px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fafafa;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#fafafa;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--blockquote-color:#222}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}time{font-family:monospace;font-style:normal;font-size:15px}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;border-radius:3px}blockquote{border-left:1px solid #999;color:var(--blockquote-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto!important}.highlight,.code{border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ class=title><h2>Keep calm and Write something</h2></a><nav><a href=/post/>Post</a>
<a href=/page/>Page</a>
<a href=/series/>Series</a>
<a href=/tags/>Tag</a>
<a href=/archive/>Archive</a>
<a href=/search/>Search</a></nav></header><main><content><h1 id=1-introduction-to-cuda-programming>1. Introduction to CUDA Programming</h1><ul><li>CPU Architecture is optimized for low latency accessing while GPU architecture is optimized for data parallel throughput compution</li><li>CPU hides latency of data by frequently stroring used data in caches and utilize the temporal locality</li><li>In CUDA, the execution unit is a warp not a thread. Context switching is happens between the warps and not threads.</li><li>GPU has lots of registers, all the thread context switching information is already present in the registers.(No context switching overhead unlike the CPU)</li><li>Host code vs. Device code.</li><li>Host memory vs. Device memory</li><li>The return type of device function is always <code>void</code>.</li><li>Data-parallel portions of an algorithm are executed on the device are kernels.</li><li>All the kernels in CUDA are asynchronous in nature. Host need to wait for the device to finish. <code>cudaDeviceSynchronize</code></li><li>Software X runs on/as HW Y<ul><li>CUDA thread &lt;-> CUDA core/SIMD code</li><li>CUDA block &lt;-> SM</li><li>Grid/kenrel &lt;-> GPU device</li></ul></li><li>One block runs on a single SM. All the threads within one block can only execute on cores in one SM.</li><li><code>&lt;&lt; BlockDim, ThreadDim >></code><ul><li><code>blockIdx</code>, <code>threadIdx</code> : Index</li><li><code>blockDim</code>, <code>threadDim</code> : Dimension (==Size)</li><li><code>blockDim</code> is the number of <strong>threads per block</strong></li></ul></li><li>Threads have mechanism to communicate and synchronize efficiently.<ul><li>The CUDA programming model allows this communication for threads whiten the same block</li><li>The therads communicate with each other <strong>in the same block</strong> using a special memory <strong>shared memory</strong></li></ul></li><li>Threads belonging to different block cannot communicate/synchronize with each other during the execution of the kernel.</li><li><code>cudaError_t e</code>. <code>cudaGetLastError</code>. Even for the multiple error, only the last one is returned. <code>a&lt;&lt;&lt; , >>>; cudaDeviceSynchronize(); e = cudaGetLastError();</code></li></ul><h2 id=backlink>Backlink</h2><p>[[Learn CUDA Programming]]</p><h2 id=date>Date</h2><p>Oct 19, 2020 4:03 PM</p><p>#cuda #book/learn-cuda-programming</p></content><p><a href=https://cychong47.github.io/tags/cuda/>#CUDA</a>
<a href=https://cychong47.github.io/tags/gpu/>#“GPU”</a>
<a href=https://cychong47.github.io/tags/book/>#“Book”</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>