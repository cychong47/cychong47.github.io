<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on Keep calm and Write something</title><link>https://cychong47.github.io/tags/kubernetes/</link><description>Recent content in Kubernetes on Keep calm and Write something</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 14 Mar 2022 00:00:03 +0900</lastBuildDate><atom:link href="https://cychong47.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>How to run DPDK in k8s - One container in each pod</title><link>https://cychong47.github.io/post/2022/2022-03-14-one-container-in-each-pod/</link><pubDate>Mon, 14 Mar 2022 00:00:03 +0900</pubDate><guid>https://cychong47.github.io/post/2022/2022-03-14-one-container-in-each-pod/</guid><description>&lt;p&gt;서로 다른 pod의 container에서 실행되는 DPDK process들도 다른 경우와 마찬가지로 DPDK runtime config 파일과 hugepage map 파일만 공유하면 hugepage를 공유할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://cychong47.github.io/images/2022/03/2022-03-14-dpdk-hugepage-1c-2p-1.png" alt="2022-03-14-dpdk-hugepage-1c-2p-1.png"&gt;&lt;/p&gt;
&lt;p&gt;이 때 서로 다른 pod가 같은 DPDK runtime config 파일들과, hugepage map 파일을 공유하기 위해 두 개 pod가 함께 사용할 수 있는 &lt;code&gt;hostPath&lt;/code&gt; 를 이용한다. &lt;code&gt;hostPath&lt;/code&gt;는 pod가 실행되는 node의 파일 시스템을 이용하여 volume을 만든다. 그러므로 서로 다른 pod가 동일 node에서 실행되는 경우에만 pod가 hugepage를 공유할 수 있다.&lt;/p&gt;</description></item><item><title>How to run DPDK in k8s - Two containers in a pod</title><link>https://cychong47.github.io/post/2022/2022-03-14-two-containers-in-a-pod/</link><pubDate>Mon, 14 Mar 2022 00:00:02 +0900</pubDate><guid>https://cychong47.github.io/post/2022/2022-03-14-two-containers-in-a-pod/</guid><description>&lt;p&gt;하나의 pod에 2개의 container를 두고, 각각의 container에 primary process, secondary process를 실행하려면 두 개 container에서 실행되는 DPDK process간 runime config 파일과, hugepage map 파일을 공유하는 방법은 다음과 같다.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://cychong47.github.io/images/2022/03/2022-03-14-dpdk-hugepage-2c-1p-1.png" alt="2022-03-14-dpdk-hugepage-2c-1p-1.png"&gt;&lt;/p&gt;
&lt;p&gt;하나의 pod에 하나의 container만 두는 경우와 달리 두 개의 container가 &lt;code&gt;/var/run/dpdk&lt;/code&gt; 위치를 공유해야 하므로, 각 container가 갖는 기본 파일 시스템이 아니라 명시적으로 pod의 volume을 이용해서 파일을 공유해야 한다.&lt;/p&gt;
&lt;p&gt;이를 위해 pod spec에 다음과 같은 volume spec을 추가한다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#f92672"&gt;volume&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; - &lt;span style="color:#f92672"&gt;name&lt;/span&gt;: &lt;span style="color:#ae81ff"&gt;dpdk-config&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#f92672"&gt;emptyDir&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;emptyDir&lt;/code&gt;에 명시적으로 지정하지 않은 경우 &lt;code&gt;tmpfs&lt;/code&gt;를 사용하므로, 위 volume은 &lt;code&gt;tmpfs&lt;/code&gt;를 사용하여 생성된다.&lt;/p&gt;</description></item><item><title>How to run DPDK in k8s - A single container in a pod</title><link>https://cychong47.github.io/post/2022/2022-03-14-a-single-container-in-a-pod/</link><pubDate>Mon, 14 Mar 2022 00:00:01 +0900</pubDate><guid>https://cychong47.github.io/post/2022/2022-03-14-a-single-container-in-a-pod/</guid><description>&lt;h2 id="how-to-run-dpdk-application-in-kubernetes-environment"&gt;How to run DPDK application in Kubernetes environment&lt;/h2&gt;
&lt;p&gt;Kubernetes에서 DPDK application을 실행하기 위해서는 DPDK application이 포함된 container에 runtime config 파일이 저장될 &lt;code&gt;/var/run/dpdk&lt;/code&gt; 디렉토리와 &lt;code&gt;hugetlbfs&lt;/code&gt; 형태로 hugepage가 존재해야 한다.
이 중 &lt;code&gt;/var/run/dpdk&lt;/code&gt;는 container 가 갖는 file system에 생성되는 기본 linux directory인 &lt;code&gt;/var/run&lt;/code&gt; 아래 위치하므로, DPDK application이 실행되면서 디렉토리를 생성한다.&lt;/p&gt;
&lt;p&gt;반면, &lt;code&gt;hugetblfs&lt;/code&gt; 디렉토리은 kubernetes의 &lt;code&gt;volume&lt;/code&gt; 에서 제공하는 &lt;code&gt;emptyDir&lt;/code&gt;을 사용하면 Pod와 lifecycle을 함께 하는 hugepage를 만들어 사용할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://cychong47.github.io/images/2022/03/2022-03-14-dpdk-hugepage-1c-1p-1.png" alt="2022-03-14-dpdk-hugepage-1c-1p-1.png"&gt;&lt;/p&gt;
&lt;p&gt;Container에서 hugepage를 할당받기 위해서는 다음과 같이 container spec에 요구하는 hugepage 크기를 명시한다.&lt;/p&gt;</description></item><item><title>Hugepage in DPDK - Basics</title><link>https://cychong47.github.io/post/2022/2022-03-14-hugepage-in-dpdk-basics/</link><pubDate>Mon, 14 Mar 2022 00:00:00 +0900</pubDate><guid>https://cychong47.github.io/post/2022/2022-03-14-hugepage-in-dpdk-basics/</guid><description>&lt;h2 id="basic-use-of-hugepage"&gt;Basic use of hugepage&lt;/h2&gt;
&lt;p&gt;&lt;img src="https://cychong47.github.io/images/2022/03/2022-03-14-dpdk-hugepage-1.png" alt="2022-03-14-dpdk-hugepage-1.png"&gt;&lt;/p&gt;
&lt;p&gt;DPDK application이 hugepage를 사용할 때 다음과 같은 2가지 디렉토리를 사용한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DPDK runtime config files
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/run/dpdk&lt;/code&gt; 로 고정된 경로를 사용요&lt;/li&gt;
&lt;li&gt;DPDK 에서 hugepage를 어떻게 사용하는 지에 대한 meta data를 저장&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hugepage map
&lt;ul&gt;
&lt;li&gt;hugepage를 사용하기 위해 &lt;code&gt;hugetlbfs&lt;/code&gt; 를 이용할 때 생성하는 hugepage map 파일들이 위치한 곳.&lt;/li&gt;
&lt;li&gt;DPDK는 이곳에 hugepage의 단위 크기를 갖는 file을 생성하고, &lt;code&gt;mmap()&lt;/code&gt;을 사용하여 hugepage를 접근&lt;/li&gt;
&lt;li&gt;만일 시스템에 2개 이상의 &lt;code&gt;hugetlbfs&lt;/code&gt;가 마운트된 위치가 있는 경우 DPDK process를 실행할 때 &lt;code&gt;--huge-dir&lt;/code&gt; 옵션을 사용하여 특정 위치를 사용하도록 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="multi-process-support"&gt;Multi-process support&lt;/h2&gt;
&lt;p&gt;&lt;img src="https://cychong47.github.io/images/2022/03/2022-03-14-dpdk-hugepage-2.png" alt="2022-03-14-dpdk-hugepage-2.png"&gt;&lt;/p&gt;</description></item><item><title>Install kubernetes on mini3</title><link>https://cychong47.github.io/post/2021/2021-04-13-install-kubernetes-on-mini3/</link><pubDate>Tue, 13 Apr 2021 11:26:01 +0900</pubDate><guid>https://cychong47.github.io/post/2021/2021-04-13-install-kubernetes-on-mini3/</guid><description>&lt;p&gt;mini1에서 mini3로의 이전을 준비 중.
기존에 mini3에는 재미삼아 k3s를 설치해 놓았는데 왠지 새로운 설정 방식을 알아야 할 필요가 있나 하는 생각이 들어 이전처럼 다시 vanilla kubernetes 를 설치하기로 했다. minkkube처럼 VM을 만들어야 설치가 되는 것도 아니고 그냥 host OS에 설치하면 되니까 설치도 간단하고(물론 바이너리 하나 설치하면 되는 k3s와는 비교하기 어렵지만) 부하를 감당하기 어려운 정도의 CPU도 아니라서.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/"&gt;Installing kubeadm | Kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;# /etc/modules-load.d/k8s.conf
br_netfilter
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;# /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;$ sudo sysctl --system
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id="install-containerd-as-a-container-runtime"&gt;Install Containerd as a Container Runtime&lt;/h1&gt;
&lt;p&gt;docker를 CRI로 사용하는 것은 곧 deprecated예정이니까 containerd를 사용해 보자.&lt;/p&gt;</description></item><item><title>Change the server IP address of k3s</title><link>https://cychong47.github.io/post/2021/2021-03-03-change-the-server-ip-address-of-k3s/</link><pubDate>Wed, 03 Mar 2021 13:17:28 +0900</pubDate><guid>https://cychong47.github.io/post/2021/2021-03-03-change-the-server-ip-address-of-k3s/</guid><description>&lt;h1 id="how-to-change-ip-address-of-k3s"&gt;How to change IP address of k3s&lt;/h1&gt;
&lt;p&gt;By default, as k3s operates in the local host, it is not possible to connect from other host.&lt;/p&gt;
&lt;p&gt;To get the server Ip address,&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;$ kubectl config view --raw |grep server
 server: https://127.0.0.1:6443
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The listening server IP address can be specified by giving parameter in running the k3s binary.&lt;/p&gt;
&lt;p&gt;K3s configuration is on &lt;code&gt;/etc/systemd/system/k3s.service&lt;/code&gt;&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;$ cat /etc/systemd/system/k3s.service
[Unit]
Description=Lightweight Kubernetes
Documentation=https://k3s.io
Wants=network-online.target
After=network-online.target

[Install]
WantedBy=multi-user.target

[Service]
Type=notify
EnvironmentFile=/etc/systemd/system/k3s.service.env
KillMode=process
Delegate=yes
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
TimeoutStartSec=0
Restart=always
RestartSec=5s
ExecStartPre=-/sbin/modprobe br_netfilter
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/k3s \
 server \
	&amp;#39;--write-kubeconfig-mode&amp;#39; \
	&amp;#39;644&amp;#39; \
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Append server IP address of &lt;code&gt;ExecStart&lt;/code&gt; option&lt;/p&gt;</description></item><item><title>How to make cronjob to support timezone</title><link>https://cychong47.github.io/post/2020/2020-10-19-how-to-fix-no-timezone-support-of-cronjob/</link><pubDate>Mon, 19 Oct 2020 14:16:45 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-10-19-how-to-fix-no-timezone-support-of-cronjob/</guid><description>&lt;h2 id="problem"&gt;Problem&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;CronJob&lt;/code&gt;이 지정된 시간에 잘 동작했는 지 확인해 본 결과 이상한 점을 발견했다.&lt;/p&gt;
&lt;p&gt;오후 2시 32분에 &lt;code&gt;CronJob&lt;/code&gt; 의 동작을 확인했는데 이전에 실행된 시간이 4시간 32분 전이라고, 즉 새벽 1시가 아니라 오전 10시에 실행이 되었다는 나오는 것이다.&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;$ date
Sat Oct 10 14:32:36 KST 2020

$ kubectl get cronjob
NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE
pocket-stat 0 1 * * * False 0 4h32m 14h
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;혹시 10시를 1시로 잘못 설정했나 하고 &lt;code&gt;kubectl describe cronjob&lt;/code&gt; 명령으로 확인해 봤지만 &lt;code&gt;schedule&lt;/code&gt; 정보는 정상적으로 설정되어 있었다는.&lt;/p&gt;</description></item><item><title>주기적으로 실행되는 앱은 CronJob으로</title><link>https://cychong47.github.io/post/2020/2020-10-09-cronjob-in-kubernetes/</link><pubDate>Fri, 09 Oct 2020 02:00:00 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-10-09-cronjob-in-kubernetes/</guid><description>&lt;p&gt;만일 &lt;code&gt;job&lt;/code&gt;을 일정 주기 혹은 특정 시간에 실행시키려면 &lt;code&gt;CronJob&lt;/code&gt; resource를 만들어 사용하면 된다.&lt;/p&gt;
&lt;h2 id="job과-cronjob간의-관계는"&gt;&lt;code&gt;Job&lt;/code&gt;과 &lt;code&gt;CronJob&lt;/code&gt;간의 관계는?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;CronJob&lt;/code&gt;에 대한 설명에 따르면 &lt;code&gt;CronJob&lt;/code&gt;정의에 기술한 특정 시간이 되면 &lt;code&gt;CronJob&lt;/code&gt;이 &lt;code&gt;Job&lt;/code&gt;을 실행한다고. 그리고 그 &lt;code&gt;Job&lt;/code&gt;이 &lt;code&gt;Pod&lt;/code&gt;를 실행한다.&lt;/p&gt;
&lt;p&gt;그럼 &lt;code&gt;Job&lt;/code&gt;을 위한 resource 정의와 &lt;code&gt;CronJob&lt;/code&gt;을 위한 resource 정의를 각각 정의해야 하나?
그렇지는 않은 듯. &lt;code&gt;CronJob&lt;/code&gt;의 정의 파일을 보면 &lt;code&gt;JobTemplate&lt;/code&gt; 항목이 &lt;code&gt;Job&lt;/code&gt;에서 볼 수 있는 &lt;code&gt;Template&lt;/code&gt;과 유사한 container spec 등을 가지고 있다. 물론 &lt;code&gt;CronJob&lt;/code&gt; 에서만 유효한 &lt;code&gt;schedule&lt;/code&gt; spec 등을 추가로 가지고 있긴 하지만.&lt;/p&gt;</description></item><item><title>일회성 앱은 Deployment가 아닌 Job으로</title><link>https://cychong47.github.io/post/2020/2020-10-09-job-instead-of-deployment/</link><pubDate>Fri, 09 Oct 2020 01:00:00 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-10-09-job-instead-of-deployment/</guid><description>&lt;p&gt;한 번 실행되면 데몬 처럼 계속해서 동작하는 앱이 아니라 필요한 일을 수행하고 종료되는 앱도 있다. 실행된 시점에 필요한 일을 수행하고 종료하는 형태로 예를 들면 특정 위치에 있는 파일을 처리하고 종료한다거나, 실행된 시점에 외부 서비스에서 필요한 정보를 가져와 어딘가 저장하는 등의 일을 하는. 이런 종류의 앱을 kubernetes에서 &lt;code&gt;Deployment&lt;/code&gt;로 배포한 경우 해당 앱은 자신이 해야 할 일을 정상적으로 수행하고 종료되지만, kubernetes scheduler 입장에서는 해당 container가 (의도하지 않게) 종료된 것으로 판단하여 다시 복구하는 절차를 수행한다. 이는 &lt;code&gt;Deployment&lt;/code&gt;로 배포된 container는 scheduler를 통해 배포된 것처럼 scheduler를 통해 제거되지 않으면 비정상이라고 판단하기 때문이다.&lt;/p&gt;</description></item><item><title>Helmize Slackbot</title><link>https://cychong47.github.io/post/2020/2020-09-10-helmize-slackbot/</link><pubDate>Thu, 10 Sep 2020 23:38:18 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-09-10-helmize-slackbot/</guid><description>&lt;p&gt;docker로 실행할 대는 환경 변수 파일에 필요한 token정보 등을 적어서 넘겼는데&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;docker run -d -p 3010:3010 --env-file=slackbot.env my-slackbot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;helm으로 할 때는 configmap을 사용하거나, value의 &lt;code&gt;env&lt;/code&gt;를 사용하거나 등등.&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the &amp;#34;infracloudio&amp;#34; chart repository
...Successfully got an update from the &amp;#34;myhelmrepo&amp;#34; chart repository
Update Complete. ⎈Happy Helming!⎈

$ helm search repo slackbot
NAME 	CHART VERSION	APP VERSION	DESCRIPTION 
myhelmrepo/slackbot	0.1.0 	1.16.0 	A Helm chart for Kubernetes
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;$ helm install -f ../../slackbot-value.yaml --version 0.1.0 slackbot myhelmrepo/slackbot
NAME: slackbot
LAST DEPLOYED: Thu Sep 10 09:47:26 2020
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
 export NODE_PORT=$(kubectl get --namespace default -o jsonpath=&amp;#34;{.spec.ports[0].nodePort}&amp;#34; services slackbot)
 export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&amp;#34;{.items[0].status.addresses[0].address}&amp;#34;)
 echo http://$NODE_IP:$NODE_PORT
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;slackbot-value.yaml&lt;/code&gt;파일의 환경 변수에 필요한 token 정보들을 기술&lt;/p&gt;</description></item><item><title>Botkube to monitor K8s cluster in Slack</title><link>https://cychong47.github.io/post/2020/2020-09-04-install-botkube/</link><pubDate>Fri, 04 Sep 2020 23:51:45 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-09-04-install-botkube/</guid><description>&lt;h1 id="install-botkube-for-k8s-and-slack"&gt;Install BotKube for k8s and slack&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.botkube.io/installation/slack/"&gt;Slack :: Messaging bot for monitoring and debugging Kubernetes clusters&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ helm repo add infracloudio https://infracloudio.github.io/charts
&amp;#34;infracloudio&amp;#34; has been added to your repositories
cychong@mini1:~$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the &amp;#34;infracloudio&amp;#34; chart repository
...Successfully got an update from the &amp;#34;myhelmrepo&amp;#34; chart repository
Update Complete. ⎈ Happy Helming!⎈
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;botkube를 위한 namespace 만들어주기&lt;/p&gt;</description></item><item><title>Upgrade Kubernetes 1 18 2</title><link>https://cychong47.github.io/post/2020/2020-05-07-upgrade-kubernetes-1-18-2/</link><pubDate>Sat, 01 Aug 2020 22:52:47 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-05-07-upgrade-kubernetes-1-18-2/</guid><description>&lt;p&gt;Upgrade kubernetes to 1.18.2&lt;/p&gt;
&lt;p&gt;Note etcd might be need to be upgrade&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ sudo kubeadm upgrade apply v1.18.2
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with &amp;#39;kubectl -n kube-system get cm kubeadm-config -oyaml&amp;#39;
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade/version] You have chosen to change the cluster version to &amp;#34;v1.18.2&amp;#34;
[upgrade/versions] Cluster version: v1.17.2
[upgrade/versions] kubeadm version: v1.18.2
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/prepull] Prepulling image for component etcd.
[upgrade/prepull] Prepulling image for component kube-apiserver.
[upgrade/prepull] Prepulling image for component kube-controller-manager.
[upgrade/prepull] Prepulling image for component kube-scheduler.
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[upgrade/prepull] Prepulled image for component etcd.
[upgrade/prepull] Prepulled image for component kube-scheduler.
[upgrade/prepull] Prepulled image for component kube-apiserver.
[upgrade/prepull] Prepulled image for component kube-controller-manager.
[upgrade/prepull] Successfully prepulled the images for all the control plane components
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version &amp;#34;v1.18.2&amp;#34;...
Static pod: kube-apiserver-mini1 hash: 4d9a965c0a14a45ea3d7db1e023096d4
Static pod: kube-controller-manager-mini1 hash: 85a33dac6d806801ba5efe4a4544194c
Static pod: kube-scheduler-mini1 hash: 9c994ea62a2d8d6f1bb7498f10aa6fcf
[upgrade/etcd] Upgrading to TLS for etcd
[upgrade/etcd] Non fatal issue encountered during upgrade: the desired etcd version for this Kubernetes version &amp;#34;v1.18.2&amp;#34; is &amp;#34;3.4.3-0&amp;#34;, but the current etcd version is &amp;#34;3.4.3&amp;#34;. Won&amp;#39;t downgrade etcd, instead just continue
[upgrade/staticpods] Writing new Static Pod manifests to &amp;#34;/etc/kubernetes/tmp/kubeadm-upgraded-manifests306630380&amp;#34;
W0502 23:43:36.998909 12626 manifests.go:225] the default kube-apiserver authorization-mode is &amp;#34;Node,RBAC&amp;#34;; using &amp;#34;Node,RBAC&amp;#34;
[upgrade/staticpods] Preparing for &amp;#34;kube-apiserver&amp;#34; upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-02-23-43-31/kube-apiserver.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-apiserver-mini1 hash: 4d9a965c0a14a45ea3d7db1e023096d4
Static pod: kube-apiserver-mini1 hash: 275339182618620ef41c93754b550d1b
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component &amp;#34;kube-apiserver&amp;#34; upgraded successfully!
[upgrade/staticpods] Preparing for &amp;#34;kube-controller-manager&amp;#34; upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-02-23-43-31/kube-controller-manager.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-controller-manager-mini1 hash: 85a33dac6d806801ba5efe4a4544194c
Static pod: kube-controller-manager-mini1 hash: 02126aeb8d0589669175da92c56e4904
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component &amp;#34;kube-controller-manager&amp;#34; upgraded successfully!
[upgrade/staticpods] Preparing for &amp;#34;kube-scheduler&amp;#34; upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-05-02-23-43-31/kube-scheduler.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-scheduler-mini1 hash: 9c994ea62a2d8d6f1bb7498f10aa6fcf
Static pod: kube-scheduler-mini1 hash: 7abb78dfbb4eae6cb52175046063ac8f
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component &amp;#34;kube-scheduler&amp;#34; upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap &amp;#34;kubeadm-config&amp;#34; in the &amp;#34;kube-system&amp;#34; Namespace
[kubelet] Creating a ConfigMap &amp;#34;kubelet-config-1.18&amp;#34; in namespace kube-system with the configuration for the kubelets in the cluster
[kubelet-start] Downloading configuration for the kubelet from the &amp;#34;kubelet-config-1.18&amp;#34; ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file &amp;#34;/var/lib/kubelet/config.yaml&amp;#34;
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to &amp;#34;v1.18.2&amp;#34;. Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven&amp;#39;t already done so.
&lt;/code&gt;&lt;/pre&gt;</description></item><item><title>K8s Vertical Pod Autoscaler</title><link>https://cychong47.github.io/post/2020/2020-07-31-vertical-pod-autoscaler/</link><pubDate>Fri, 31 Jul 2020 00:01:03 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-07-31-vertical-pod-autoscaler/</guid><description>&lt;p&gt;VPA는 K8s에서 &lt;code&gt;Scale-in/out&lt;/code&gt;에 대한 기능을 제공.&lt;/p&gt;
&lt;p&gt;현재 동작하고 있는 pod에 대해 pod restart 없이 할당된 resource를 변경하는 것은 아직 미지원인듯&lt;/p&gt;
&lt;h2 id="github의-vpa-repo"&gt;github의 VPA repo&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler"&gt;https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Updating running pods is an experimental feature of VPA. Whenever VPA updates the pod resources the pod is recreated, which causes all running containers to be restarted. The pod may be recreated on a different node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://stupefied-goodall-e282f7.netlify.app/contributors/design-proposals/autoscaling/vertical-pod-autoscaler/"&gt;https://stupefied-goodall-e282f7.netlify.app/contributors/design-proposals/autoscaling/vertical-pod-autoscaler/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In-place updates&lt;/p&gt;
&lt;p&gt;In-place Pod updates (#5774) is a planned feature to allow changing resources (request/limit) of existing containers without killing them, assuming sufficient free resources available on the node. Vertical Pod Autoscaler will greatly benefit from this ability, however it is not considered a blocker for the MVP.&lt;/p&gt;</description></item><item><title>Deploy nginx with helm</title><link>https://cychong47.github.io/post/2020/2020-07-02-deploy-nginx-with-helm/</link><pubDate>Thu, 02 Jul 2020 23:58:41 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-07-02-deploy-nginx-with-helm/</guid><description>&lt;p&gt;우선 &lt;code&gt;docker&lt;/code&gt;로 실행한 nginx container를 종료시키고&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~/work/helm-chart-github$ docker ps -a |grep nginx
a66786635c60 nginx &amp;#34;/docker-entrypoint.…&amp;#34; 7 minutes ago Up 7 minutes k8s_nginx_my-nginx-77596b9fc6-7txns_default_44840d63-b496-4a58-9e18-83e503c6d2cf_0
85c85322ea59 k8s.gcr.io/pause:3.1 &amp;#34;/pause&amp;#34; 7 minutes ago Up 7 minutes k8s_POD_my-nginx-77596b9fc6-7txns_default_44840d63-b496-4a58-9e18-83e503c6d2cf_0
5be06dc3b184 nginx &amp;#34;nginx -g &amp;#39;daemon of…&amp;#34; 2 weeks ago Up 2 weeks 0.0.0.0:8099-&amp;gt;80/tcp podcast
2812c510a5b6 nginx &amp;#34;nginx -g &amp;#39;daemon of…&amp;#34; 2 weeks ago Up 2 weeks 0.0.0.0:80-&amp;gt;80/tcp sosa0sa
cychong@mini1:~/work/helm-chart-github$ docker stop 5be06dc3b184
5be06dc3b184
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;nginx를 구동시킬 helm chart 준비&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~/work/helm-chart/nginx$ tree -f
.
├── ./Chart.yaml
├── ./README.md
├── ./charts
├── ./nginx-pv.yaml
├── ./templates
│   ├── ./templates/NOTES.txt
│   ├── ./templates/_helpers.tpl
│   ├── ./templates/deployment.yaml
│   ├── ./templates/ingress.yaml
│   ├── ./templates/pvc.yaml
│   ├── ./templates/service.yaml
│   └── ./templates/tests
│   └── ./templates/tests/test-connection.yaml
└── ./values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;우선 PV(&lt;code&gt;Persistent Volume&lt;/code&gt;) 생성.&lt;/p&gt;</description></item><item><title>Upgrade kubenetes to 1.17.2-0</title><link>https://cychong47.github.io/post/2020/new-things-in-last-4-years/</link><pubDate>Thu, 30 Jan 2020 00:00:00 +0900</pubDate><guid>https://cychong47.github.io/post/2020/new-things-in-last-4-years/</guid><description>&lt;h1 id="preparation"&gt;preparation&lt;/h1&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ sudo apt update
[sudo] password for cychong:
Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease
Get:2 http://dl.google.com/linux/chrome/deb stable Release [943 B]
Get:3 http://dl.google.com/linux/chrome/deb stable Release.gpg [819 B]
...
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id="get-the-lastest-version"&gt;Get the lastest version&lt;/h1&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ sudo apt-cache policy kubeadm
kubeadm:
 Installed: 1.16.1-00
 Candidate: 1.17.2-00
 Version table:
 1.17.2-00 500
 500 http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 1.17.1-00 500
 500 http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 1.17.0-00 500
 500 http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
...
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ apt-cache madison kubeadm
 kubeadm | 1.17.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.17.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.17.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.16.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.16.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.16.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.16.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.16.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.16.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.16.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.15.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.14.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.12-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.11-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.13.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.12.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.11.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.13-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.12-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.11-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.10.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.11-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.9.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.15-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.14-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.13-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.12-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.11-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.1-01 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.0-01 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.8.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.16-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.15-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.14-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.11-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.3-01 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.7.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.13-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.12-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.11-01 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.6.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
 kubeadm | 1.5.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
cychong@mini1:~$ sudo apt-mark unhold kubeadm &amp;amp;&amp;amp; sudo apt-get install -y kubeadm=1.17.2-00 &amp;amp;&amp;amp; sudo apt-mark hold kubeadm
Canceled hold on kubeadm.
Reading package lists... Done
Building dependency tree 
Reading state information... Done
The following packages will be upgraded:
 kubeadm
1 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.
Need to get 8061 kB of archives.
After this operation, 4907 kB disk space will be freed.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.17.2-00 [8061 kB]
Fetched 8061 kB in 3s (3164 kB/s) 
(Reading database ... 248932 files and directories currently installed.)
Preparing to unpack .../kubeadm_1.17.2-00_amd64.deb ...
Unpacking kubeadm (1.17.2-00) over (1.16.1-00) ...
Setting up kubeadm (1.17.2-00) ...
kubeadm set on hold.
cychong@mini1:~$ sudo apt-get update &amp;amp;&amp;amp; apt-get install -y --allow-change-held-packages kubeadm=1.17.2-00
Ign:2 http://dl.google.com/linux/chrome/deb stable InRelease 
Hit:3 https://download.docker.com/linux/ubuntu bionic InRelease 
Hit:4 http://dl.google.com/linux/chrome/deb stable Release 
Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease 
Hit:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease 
Hit:8 http://ppa.launchpad.net/x2go/stable/ubuntu bionic InRelease 
Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease 
Hit:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease 
Hit:10 http://archive.ubuntu.com/ubuntu bionic-security InRelease 
Reading package lists... Done 
E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?
cychong@mini1:~$ sudo apt-get install -y --allow-change-held-packages kubeadm=1.17.2-00
Reading package lists... Done
Building dependency tree 
Reading state information... Done
kubeadm is already the newest version (1.17.2-00).
0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.
cychong@mini1:~$ kubeadm version
kubeadm version: &amp;amp;version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;17&amp;#34;, GitVersion:&amp;#34;v1.17.2&amp;#34;, GitCommit:&amp;#34;59603c6e503c87169aea6106f57b9f242f64df89&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-01-18T23:27:49Z&amp;#34;, GoVersion:&amp;#34;go1.13.5&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;}
cychong@mini1:~$ sudo kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with &amp;#39;kubectl -n kube-system get cm kubeadm-config -oyaml&amp;#39;
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.16.1
[upgrade/versions] kubeadm version: v1.17.2
[upgrade/versions] Latest stable version: v1.17.2
[upgrade/versions] Latest version in the v1.16 series: v1.16.6

Components that must be upgraded manually after you have upgraded the control plane with &amp;#39;kubeadm upgrade apply&amp;#39;:
COMPONENT CURRENT AVAILABLE
Kubelet 1 x v1.16.1 v1.16.6

Upgrade to the latest version in the v1.16 series:

COMPONENT CURRENT AVAILABLE
API Server v1.16.1 v1.16.6
Controller Manager v1.16.1 v1.16.6
Scheduler v1.16.1 v1.16.6
Kube Proxy v1.16.1 v1.16.6
CoreDNS 1.6.2 1.6.5
Etcd 3.3.15 3.3.17-0

You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.16.6

_____________________________________________________________________

Components that must be upgraded manually after you have upgraded the control plane with &amp;#39;kubeadm upgrade apply&amp;#39;:
COMPONENT CURRENT AVAILABLE
Kubelet 1 x v1.16.1 v1.17.2

Upgrade to the latest stable version:

COMPONENT CURRENT AVAILABLE
API Server v1.16.1 v1.17.2
Controller Manager v1.16.1 v1.17.2
Scheduler v1.16.1 v1.17.2
Kube Proxy v1.16.1 v1.17.2
CoreDNS 1.6.2 1.6.5
Etcd 3.3.15 3.4.3-0

You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.17.2

_____________________________________________________________________

cychong@mini1:~$ sudo kubeadm upgrade apply v1.17.2
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with &amp;#39;kubectl -n kube-system get cm kubeadm-config -oyaml&amp;#39;
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/version] You have chosen to change the cluster version to &amp;#34;v1.17.2&amp;#34;
[upgrade/versions] Cluster version: v1.16.1
[upgrade/versions] kubeadm version: v1.17.2
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/prepull] Prepulling image for component etcd.
[upgrade/prepull] Prepulling image for component kube-apiserver.
[upgrade/prepull] Prepulling image for component kube-controller-manager.
[upgrade/prepull] Prepulling image for component kube-scheduler.
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd
[upgrade/prepull] Prepulled image for component kube-controller-manager.
[upgrade/prepull] Prepulled image for component kube-apiserver.
[upgrade/prepull] Prepulled image for component kube-scheduler.
[upgrade/prepull] Prepulled image for component etcd.
[upgrade/prepull] Successfully prepulled the images for all the control plane components
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version &amp;#34;v1.17.2&amp;#34;...
Static pod: kube-apiserver-mini1 hash: 66d5b6802b69fcb461e22c159ef72783
Static pod: kube-controller-manager-mini1 hash: 98ded181cb6da00c408078fe0832bddf
Static pod: kube-scheduler-mini1 hash: e05eb744bc3406614b4a55dd00e7af9f
[upgrade/etcd] Upgrading to TLS for etcd
Static pod: etcd-mini1 hash: 9e59bd8449d154ddd6acfbbb3a74181f
[upgrade/staticpods] Preparing for &amp;#34;etcd&amp;#34; upgrade
[upgrade/staticpods] Renewing etcd-server certificate
[upgrade/staticpods] Renewing etcd-peer certificate
[upgrade/staticpods] Renewing etcd-healthcheck-client certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/etcd.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-01-30-22-21-26/etcd.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: etcd-mini1 hash: 9e59bd8449d154ddd6acfbbb3a74181f
Static pod: etcd-mini1 hash: 9b305733637de70ef82ca5b0b18c65e1
[apiclient] Found 1 Pods for label selector component=etcd
[upgrade/staticpods] Component &amp;#34;etcd&amp;#34; upgraded successfully!
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to &amp;#34;/etc/kubernetes/tmp/kubeadm-upgraded-manifests216455436&amp;#34;
W0130 22:22:01.351368 10213 manifests.go:214] the default kube-apiserver authorization-mode is &amp;#34;Node,RBAC&amp;#34;; using &amp;#34;Node,RBAC&amp;#34;
[upgrade/staticpods] Preparing for &amp;#34;kube-apiserver&amp;#34; upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-01-30-22-21-26/kube-apiserver.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-apiserver-mini1 hash: 66d5b6802b69fcb461e22c159ef72783
Static pod: kube-apiserver-mini1 hash: 4d9a965c0a14a45ea3d7db1e023096d4
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component &amp;#34;kube-apiserver&amp;#34; upgraded successfully!
[upgrade/staticpods] Preparing for &amp;#34;kube-controller-manager&amp;#34; upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-01-30-22-21-26/kube-controller-manager.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-controller-manager-mini1 hash: 98ded181cb6da00c408078fe0832bddf
Static pod: kube-controller-manager-mini1 hash: 98ded181cb6da00c408078fe0832bddf
Static pod: kube-controller-manager-mini1 hash: 85a33dac6d806801ba5efe4a4544194c
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component &amp;#34;kube-controller-manager&amp;#34; upgraded successfully!
[upgrade/staticpods] Preparing for &amp;#34;kube-scheduler&amp;#34; upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-01-30-22-21-26/kube-scheduler.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-scheduler-mini1 hash: e05eb744bc3406614b4a55dd00e7af9f
Static pod: kube-scheduler-mini1 hash: 9c994ea62a2d8d6f1bb7498f10aa6fcf
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component &amp;#34;kube-scheduler&amp;#34; upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap &amp;#34;kubeadm-config&amp;#34; in the &amp;#34;kube-system&amp;#34; Namespace
[kubelet] Creating a ConfigMap &amp;#34;kubelet-config-1.17&amp;#34; in namespace kube-system with the configuration for the kubelets in the cluster
[kubelet-start] Downloading configuration for the kubelet from the &amp;#34;kubelet-config-1.17&amp;#34; ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file &amp;#34;/var/lib/kubelet/config.yaml&amp;#34;
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons]: Migrating CoreDNS Corefile
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to &amp;#34;v1.17.2&amp;#34;. Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven&amp;#39;t already done so.
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id="calico"&gt;calico&lt;/h1&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ wget https://docs.projectcalico.org/manifests/calico.yaml
--2020-01-30 22:34:28-- https://docs.projectcalico.org/manifests/calico.yaml
Resolving docs.projectcalico.org (docs.projectcalico.org)... 206.189.89.118, 2400:6180:0:d1::57a:6001
Connecting to docs.projectcalico.org (docs.projectcalico.org)|206.189.89.118|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 20935 (20K) [application/x-yaml]
Saving to: ‘calico.yaml.2’

calico.yaml.2 100%[===========================================================================================================================================&amp;gt;] 20.44K --.-KB/s in 0.07s 

2020-01-30 22:34:29 (275 KB/s) - ‘calico.yaml.2’ saved [20935/20935]
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;618 - name: CALICO_IPV4POOL_CIDR
619 value: &amp;#34;10.201.0.0/24&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ kubectl apply -f calico.yaml
configmap/calico-config unchanged
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org unchanged
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrole.rbac.authorization.k8s.io/calico-node configured
clusterrolebinding.rbac.authorization.k8s.io/calico-node unchanged
daemonset.apps/calico-node configured
serviceaccount/calico-node unchanged
deployment.apps/calico-kube-controllers configured
serviceaccount/calico-kube-controllers unchanged
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id="kubelet-and-kubectl"&gt;kubelet and kubectl&lt;/h1&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ sudo apt-mark unhold kubelet kubectl &amp;amp;&amp;amp; sudo apt-get install -y kubelet=1.17.2-00 kubectl=1.17.2-00 &amp;amp;&amp;amp; sudo apt-mark hold kubelet kubectl
kubelet was already not hold.
kubectl was already not hold.
Reading package lists... Done
Building dependency tree 
Reading state information... Done
The following packages will be upgraded:
 kubectl kubelet
2 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.
Need to get 27.9 MB of archives.
After this operation, 14.8 MB disk space will be freed.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubectl amd64 1.17.2-00 [8738 kB]
Get:2 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.17.2-00 [19.2 MB]
Fetched 27.9 MB in 6s (4994 kB/s) 
(Reading database ... 248932 files and directories currently installed.)
Preparing to unpack .../kubectl_1.17.2-00_amd64.deb ...
Unpacking kubectl (1.17.2-00) over (1.16.1-00) ...
Preparing to unpack .../kubelet_1.17.2-00_amd64.deb ...
Unpacking kubelet (1.17.2-00) over (1.16.1-00) ...
Setting up kubelet (1.17.2-00) ...
Setting up kubectl (1.17.2-00) ...
kubelet set on hold.
kubectl set on hold.
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id="wrap-up"&gt;wrap-up&lt;/h1&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ sudo systemctl restart kubelet
cychong@mini1:~$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
mini1 Ready master 144d v1.17.2
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id="reference"&gt;reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/"&gt;https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Upgrade ghost to v3.0</title><link>https://cychong47.github.io/post/2019/upgrade-ghost-to-v3-0/</link><pubDate>Tue, 05 Nov 2019 14:58:16 +0900</pubDate><guid>https://cychong47.github.io/post/2019/upgrade-ghost-to-v3-0/</guid><description>&lt;ul&gt;
&lt;li&gt;부제 1. pod가 동작하지 않을때 원인 찾기&lt;/li&gt;
&lt;li&gt;부제 2. helm upgrade 명령을 이용하여 업데이트 하기&lt;/li&gt;
&lt;li&gt;부제 3. sqlite를 이용해서 ghost.db 직접 수정하기&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;values.yaml에 명시되어 있는 ghost docker image의 버전 정보를 2.31.0에서 3.0.2 최신 버전으로 업데이트 후 아래 명령어로 업데이트&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;$ helm upgrade --debug my-ghost ghost-with-helm
[debug] Created tunnel using local port: &amp;#39;45263&amp;#39;

[debug] SERVER: &amp;#34;127.0.0.1:45263&amp;#34;

REVISION: 6
RELEASED: Tue Nov 5 22:25:19 2019
CHART: ghost-0.1.0
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
env:
 node_env: production
 url: http://sosa0sa.com:2368
fullnameOverride: &amp;#34;&amp;#34;
image:
 pullPolicy: Always
 registry: docekr.io
 repository: ghost
 tag: 3.0.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Helm upgrade는 완료되었지만 ghost 블로그가 접속이 되지 않는다. 상태를 확인해 보니&lt;/p&gt;</description></item><item><title>kubelet이 실행되지 않을때</title><link>https://cychong47.github.io/post/2019/what-if-kubelet-is-not-started/</link><pubDate>Tue, 05 Nov 2019 14:43:35 +0900</pubDate><guid>https://cychong47.github.io/post/2019/what-if-kubelet-is-not-started/</guid><description>&lt;p&gt;mini1 리붓 후 ghost 접속이 안됨.&lt;/p&gt;
&lt;p&gt;docker를 직접 실행시키는 wordpress는 정상적으로 실행&lt;/p&gt;
&lt;p&gt;그래서 kubectl get svc 명령을 치니 접속이 안된다고.&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;$ ps -ef |grep kube
cychong 7461 2486 0 23:26 pts/0 00:00:00 grep --color=auto kube
$ service kubelet status
● kubelet.service - kubelet: The Kubernetes Node Agent
 Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
 Drop-In: /etc/systemd/system/kubelet.service.d
 └─10-kubeadm.conf
 Active: activating (auto-restart) (Result: exit-code) since Mon 2019-11-04 23:26:47 KST; 5s ago
 Docs: https://kubernetes.io/docs/home/
 Process: 7664 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=255
 Main PID: 7664 (code=exited, status=255)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;$ journalctl -xeu kubelet
Nov 04 23:28:32 mini1 kubelet[8695]: I1104 23:28:32.418274 8695 server.go:773] Client rotation is on, will bootstrap in background
Nov 04 23:28:32 mini1 kubelet[8695]: I1104 23:28:32.427223 8695 certificate_store.go:129] Loading cert/key pair from &amp;#34;/var/lib/kubelet/pki/kubelet-clien
Nov 04 23:28:32 mini1 kubelet[8695]: I1104 23:28:32.580296 8695 server.go:644] --cgroups-per-qos enabled, but --cgroup-root was not specified. defaulti
Nov 04 23:28:32 mini1 kubelet[8695]: F1104 23:28:32.580915 8695 server.go:271] failed to run Kubelet: running with swap on is not supported, please disa
Nov 04 23:28:32 mini1 systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
Nov 04 23:28:32 mini1 systemd[1]: kubelet.service: Failed with result &amp;#39;exit-code&amp;#39;.
Nov 04 23:28:42 mini1 systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Nov 04 23:28:42 mini1 systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 30.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: http://www.ubuntu.com/support
--
-- Automatic restarting of the unit kubelet.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Nov 04 23:28:42 mini1 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
-- Subject: Unit kubelet.service has finished shutting down
-- Defined-By: systemd
-- Support: http://www.ubuntu.com/support
--
-- Unit kubelet.service has finished shutting down.
Nov 04 23:28:42 mini1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
-- Subject: Unit kubelet.service has finished start-up
-- Defined-By: systemd
-- Support: http://www.ubuntu.com/support
--
-- Unit kubelet.service has finished starting up.
--
-- The start-up result is RESULT.
Nov 04 23:28:42 mini1 kubelet[8795]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet&amp;#39;s
Nov 04 23:28:42 mini1 kubelet[8795]: Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet&amp;#39;s --
Nov 04 23:28:42 mini1 kubelet[8795]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet&amp;#39;s
Nov 04 23:28:42 mini1 kubelet[8795]: Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet&amp;#39;s --
Nov 04 23:28:42 mini1 kubelet[8795]: I1104 23:28:42.907863 8795 server.go:410] Version: v1.16.1
Nov 04 23:28:42 mini1 kubelet[8795]: I1104 23:28:42.908251 8795 plugins.go:100] No cloud provider specified.
Nov 04 23:28:42 mini1 kubelet[8795]: I1104 23:28:42.908283 8795 server.go:773] Client rotation is on, will bootstrap in background
Nov 04 23:28:42 mini1 kubelet[8795]: I1104 23:28:42.917653 8795 certificate_store.go:129] Loading cert/key pair from &amp;#34;/var/lib/kubelet/pki/kubelet-clien
Nov 04 23:28:43 mini1 kubelet[8795]: I1104 23:28:43.073234 8795 server.go:644] --cgroups-per-qos enabled, but --cgroup-root was not specified. defaulti
Nov 04 23:28:43 mini1 kubelet[8795]: F1104 23:28:43.073886 8795 server.go:271] failed to run Kubelet: running with swap on is not supported, please disa
Nov 04 23:28:43 mini1 systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
Nov 04 23:28:43 mini1 systemd[1]: kubelet.service: Failed with result &amp;#39;exit-code&amp;#39;.
lines 1947-1986/1986 (END)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;찾아보니 &lt;code&gt;docker&lt;/code&gt;가 정상적으로 실행 중인지 확인해 보라고.&lt;/p&gt;</description></item><item><title>Enable SCTP in kubernetes</title><link>https://cychong47.github.io/post/2019/enable-sctp-in-kubernetes/</link><pubDate>Mon, 07 Oct 2019 15:39:35 +0900</pubDate><guid>https://cychong47.github.io/post/2019/enable-sctp-in-kubernetes/</guid><description>&lt;h2 id="check-if-sctp-is-supported-by-creating-sctp-service"&gt;Check if SCTP is supported by creating SCTP service&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://blog.aweimeow.tw/enable-sctp-in-kubernetes-cluster/"&gt;https://blog.aweimeow.tw/enable-sctp-in-kubernetes-cluster/&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~/work/sctp$ cat service.yaml
apiVersion: v1
kind: Service
metadata:
 name: sctp
spec:
 selector:
 app: sctp
 ports:
 - protocol: SCTP
 port: 9999
 targetPort: 30001
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~/work/sctp$ kubectl create -f service.yaml
The Service &amp;#34;sctp&amp;#34; is invalid: spec.ports[0].protocol: Unsupported value: &amp;#34;SCTP&amp;#34;: supported values: &amp;#34;TCP&amp;#34;, &amp;#34;UDP&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id="enable-sctp-in-running-kubernetes-cluster"&gt;Enable SCTP in running kubernetes cluster&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/55909512/how-to-configure-already-running-cluster-in-kubernetes"&gt;https://stackoverflow.com/questions/55909512/how-to-configure-already-running-cluster-in-kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Basically you must pass this flag to kube-apiserver. How you can do that depends on how you set up the cluster. If you used kubeadm or kubespray then you should edit file &lt;code&gt;/etc/kubernetes/manifests/kube-apiserver.yaml&lt;/code&gt; and add this flag somewhere under &amp;ldquo;command&amp;rdquo; field (somewhere between other flags). After that kube-apiserver pod should be restarted automatically. If not - you can kill it by hand.&lt;/p&gt;</description></item><item><title>Upgrade kubernetes to 1.16.1</title><link>https://cychong47.github.io/post/2019/upgrade-kubernetes/</link><pubDate>Mon, 07 Oct 2019 15:37:14 +0900</pubDate><guid>https://cychong47.github.io/post/2019/upgrade-kubernetes/</guid><description>&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~/work/ghost-with-helm-x$ sudo apt update
[sudo] password for cychong:
Ign:2 http://dl.google.com/linux/chrome/deb stable InRelease
Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease
Hit:4 https://download.docker.com/linux/ubuntu bionic InRelease
Hit:5 http://dl.google.com/linux/chrome/deb stable Release
Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
Hit:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease
Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
Get:9 http://archive.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 DEP-11 Metadata [295 kB]
Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main DEP-11 48x48 Icons [73.8 kB]
Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main DEP-11 64x64 Icons [147 kB]
Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 DEP-11 Metadata [254 kB]
Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe DEP-11 48x48 Icons [197 kB]
Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe DEP-11 64x64 Icons [453 kB]
Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 DEP-11 Metadata [2468 B]
Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 DEP-11 Metadata [7916 B]
Get:18 http://archive.ubuntu.com/ubuntu bionic-security/main amd64 Packages [526 kB]
Get:19 http://archive.ubuntu.com/ubuntu bionic-security/main Translation-en [176 kB]
Get:20 http://archive.ubuntu.com/ubuntu bionic-security/main amd64 DEP-11 Metadata [38.5 kB]
Get:21 http://archive.ubuntu.com/ubuntu bionic-security/main DEP-11 48x48 Icons [17.6 kB]
Get:22 http://archive.ubuntu.com/ubuntu bionic-security/main DEP-11 64x64 Icons [41.5 kB]
Get:23 http://archive.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [611 kB]
Get:24 http://archive.ubuntu.com/ubuntu bionic-security/universe Translation-en [203 kB]
Get:25 http://archive.ubuntu.com/ubuntu bionic-security/universe amd64 DEP-11 Metadata [42.2 kB]
Get:26 http://archive.ubuntu.com/ubuntu bionic-security/universe DEP-11 48x48 Icons [16.4 kB]
Get:27 http://archive.ubuntu.com/ubuntu bionic-security/universe DEP-11 64x64 Icons [111 kB]
Get:28 http://archive.ubuntu.com/ubuntu bionic-security/multiverse amd64 DEP-11 Metadata [2464 B]
Fetched 3467 kB in 27s (128 kB/s)
Reading package lists... Done
Building dependency tree
Reading state information... Done
41 packages can be upgraded. Run &amp;#39;apt list --upgradable&amp;#39; to see them.

cychong@mini1:~/work/ghost-with-helm-x$ sudo apt-cache policy kubeadm
kubeadm:
 Installed: 1.15.3-00
 Candidate: 1.16.1-00
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~/work/ghost-with-helm-x$ sudo apt-get install -y kubeadm=1.16.1-00 &amp;amp;&amp;amp; sudo apt-mark hold kubeadm
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following package was automatically installed and is no longer required:
 libllvm7
Use &amp;#39;sudo apt autoremove&amp;#39; to remove it.
The following packages will be upgraded:
 kubeadm
1 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.
Need to get 8764 kB of archives.
After this operation, 4062 kB of additional disk space will be used.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.16.1-00 [8764 kB]
Fetched 8764 kB in 6s (1489 kB/s)
(Reading database ... 237550 files and directories currently installed.)
Preparing to unpack .../kubeadm_1.16.1-00_amd64.deb ...
Unpacking kubeadm (1.16.1-00) over (1.15.3-00) ...
Setting up kubeadm (1.16.1-00) ...
kubeadm set on hold.

cychong@mini1:~/work/ghost-with-helm-x$ kubeadm version
kubeadm version: &amp;amp;version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;16&amp;#34;, GitVersion:&amp;#34;v1.16.1&amp;#34;, GitCommit:&amp;#34;d647ddbd755faf07169599a625faf302ffc34458&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2019-10-02T16:58:27Z&amp;#34;, GoVersion:&amp;#34;go1.12.10&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;}

cychong@mini1:~/work/ghost-with-helm-x$ sudo kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with &amp;#39;kubectl -n kube-system get cm kubeadm-config -oyaml&amp;#39;
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.15.3
[upgrade/versions] kubeadm version: v1.16.1
[upgrade/versions] Latest stable version: v1.16.1
[upgrade/versions] Latest version in the v1.15 series: v1.15.4

Components that must be upgraded manually after you have upgraded the control plane with &amp;#39;kubeadm upgrade apply&amp;#39;:
COMPONENT CURRENT AVAILABLE
Kubelet 1 x v1.15.3 v1.15.4

Upgrade to the latest version in the v1.15 series:

COMPONENT CURRENT AVAILABLE
API Server v1.15.3 v1.15.4
Controller Manager v1.15.3 v1.15.4
Scheduler v1.15.3 v1.15.4
Kube Proxy v1.15.3 v1.15.4
CoreDNS 1.3.1 1.6.2
Etcd 3.3.10 3.3.10

You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.15.4

_____________________________________________________________________

Components that must be upgraded manually after you have upgraded the control plane with &amp;#39;kubeadm upgrade apply&amp;#39;:
COMPONENT CURRENT AVAILABLE
Kubelet 1 x v1.15.3 v1.16.1

Upgrade to the latest stable version:

COMPONENT CURRENT AVAILABLE
API Server v1.15.3 v1.16.1
Controller Manager v1.15.3 v1.16.1
Scheduler v1.15.3 v1.16.1
Kube Proxy v1.15.3 v1.16.1
CoreDNS 1.3.1 1.6.2
Etcd 3.3.10 3.3.15-0

You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.16.1

_____________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id="failed"&gt;failed&lt;/h3&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~/work/ghost-with-helm-x$ sudo kubeadm upgrade apply v1.16.1
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with &amp;#39;kubectl -n kube-system get cm kubeadm-config -oyaml&amp;#39;
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/version] You have chosen to change the cluster version to &amp;#34;v1.16.1&amp;#34;
[upgrade/versions] Cluster version: v1.15.3
[upgrade/versions] kubeadm version: v1.16.1
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/prepull] Prepulling image for component etcd.
[upgrade/prepull] Prepulling image for component kube-controller-manager.
[upgrade/prepull] Prepulling image for component kube-apiserver.
[upgrade/prepull] Prepulling image for component kube-scheduler.
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver


[upgrade/prepull] Failed prepulled the images for the control plane components error: the prepull operation timed out
To see the stack trace of this error execute with --v=5 or higher
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id="retry"&gt;retry&lt;/h3&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ sudo kubeadm upgrade apply v1.16.1
[sudo] password for cychong:
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with &amp;#39;kubectl -n kube-system get cm kubeadm-config -oyaml&amp;#39;
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/version] You have chosen to change the cluster version to &amp;#34;v1.16.1&amp;#34;
[upgrade/versions] Cluster version: v1.15.3
[upgrade/versions] kubeadm version: v1.16.1
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/prepull] Prepulling image for component etcd.
[upgrade/prepull] Prepulling image for component kube-controller-manager.
[upgrade/prepull] Prepulling image for component kube-scheduler.
[upgrade/prepull] Prepulling image for component kube-apiserver.
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[upgrade/prepull] Prepulled image for component kube-apiserver.
[upgrade/prepull] Prepulled image for component etcd.
[upgrade/prepull] Prepulled image for component kube-controller-manager.
[upgrade/prepull] Prepulled image for component kube-scheduler.
[upgrade/prepull] Successfully prepulled the images for all the control plane components
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version &amp;#34;v1.16.1&amp;#34;...
Static pod: kube-apiserver-mini1 hash: 868871559cc75dab75f106d4af342538
Static pod: kube-controller-manager-mini1 hash: 44f6b9cce90e81a472520a3fb9751d10
Static pod: kube-scheduler-mini1 hash: 7d5d3c0a6786e517a8973fa06754cb75
[upgrade/etcd] Upgrading to TLS for etcd
Static pod: etcd-mini1 hash: 71542aaa2829652ef14b22098a4b46aa
[upgrade/staticpods] Preparing for &amp;#34;etcd&amp;#34; upgrade
[upgrade/staticpods] Renewing etcd-server certificate
[upgrade/staticpods] Renewing etcd-peer certificate
[upgrade/staticpods] Renewing etcd-healthcheck-client certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/etcd.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-10-07-23-43-23/etcd.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: etcd-mini1 hash: 71542aaa2829652ef14b22098a4b46aa
Static pod: etcd-mini1 hash: 71542aaa2829652ef14b22098a4b46aa
Static pod: etcd-mini1 hash: 71542aaa2829652ef14b22098a4b46aa
Static pod: etcd-mini1 hash: 71542aaa2829652ef14b22098a4b46aa
Static pod: etcd-mini1 hash: 71542aaa2829652ef14b22098a4b46aa
Static pod: etcd-mini1 hash: 71542aaa2829652ef14b22098a4b46aa
Static pod: etcd-mini1 hash: 71542aaa2829652ef14b22098a4b46aa
Static pod: etcd-mini1 hash: 71542aaa2829652ef14b22098a4b46aa
Static pod: etcd-mini1 hash: d96090bab45a5dababb3c3015960926b
[apiclient] Found 1 Pods for label selector component=etcd
[upgrade/staticpods] Component &amp;#34;etcd&amp;#34; upgraded successfully!
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to &amp;#34;/etc/kubernetes/tmp/kubeadm-upgraded-manifests306281752&amp;#34;
[upgrade/staticpods] Preparing for &amp;#34;kube-apiserver&amp;#34; upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-10-07-23-43-23/kube-apiserver.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-apiserver-mini1 hash: 868871559cc75dab75f106d4af342538
Static pod: kube-apiserver-mini1 hash: 01800dd11dfbda441372caf7cbf8aa39
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component &amp;#34;kube-apiserver&amp;#34; upgraded successfully!
[upgrade/staticpods] Preparing for &amp;#34;kube-controller-manager&amp;#34; upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-10-07-23-43-23/kube-controller-manager.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-controller-manager-mini1 hash: 44f6b9cce90e81a472520a3fb9751d10
Static pod: kube-controller-manager-mini1 hash: e12d193633dcf11f6095d89ee58c45a9
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component &amp;#34;kube-controller-manager&amp;#34; upgraded successfully!
[upgrade/staticpods] Preparing for &amp;#34;kube-scheduler&amp;#34; upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to &amp;#34;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;#34; and backed up old manifest to &amp;#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-10-07-23-43-23/kube-scheduler.yaml&amp;#34;
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-scheduler-mini1 hash: 7d5d3c0a6786e517a8973fa06754cb75
Static pod: kube-scheduler-mini1 hash: bf9014e67294b0df0bc373fd7024ced7
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component &amp;#34;kube-scheduler&amp;#34; upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap &amp;#34;kubeadm-config&amp;#34; in the &amp;#34;kube-system&amp;#34; Namespace
[kubelet] Creating a ConfigMap &amp;#34;kubelet-config-1.16&amp;#34; in namespace kube-system with the configuration for the kubelets in the cluster
[kubelet-start] Downloading configuration for the kubelet from the &amp;#34;kubelet-config-1.16&amp;#34; ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file &amp;#34;/var/lib/kubelet/config.yaml&amp;#34;
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to &amp;#34;v1.16.1&amp;#34;. Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven&amp;#39;t already done so.
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id="upgrade-calico-from-v38-to-v39"&gt;Upgrade calico from v3.8 to v3.9&lt;/h3&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;kubectl apply -f https://docs.projectcalico.org/v3.9/manifests/calico.yaml
configmap/calico-config unchanged
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org unchanged
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrole.rbac.authorization.k8s.io/calico-node configured
clusterrolebinding.rbac.authorization.k8s.io/calico-node unchanged
daemonset.apps/calico-node configured
serviceaccount/calico-node unchanged
deployment.apps/calico-kube-controllers configured
serviceaccount/calico-kube-controllers unchanged
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;앗. 서브넷 변경하는 걸 깜빡&lt;/p&gt;</description></item><item><title>Setup kubernetes in a single host</title><link>https://cychong47.github.io/post/2019/setup-kubernetes-with-a-single-host/</link><pubDate>Mon, 23 Sep 2019 15:03:13 +0900</pubDate><guid>https://cychong47.github.io/post/2019/setup-kubernetes-with-a-single-host/</guid><description>&lt;p&gt;Replace microk8s with kubernetes in mini1&lt;/p&gt;
&lt;h1 id="remove-microk8s-with-snap-command"&gt;remove micro.k8s with snap command&lt;/h1&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ sudo snap remove microk8s
Save data of snap &amp;#34;microk8s&amp;#34; in automatic snapshot set 
microk8s removed
cychong@mini1:~$
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id="setup-kubernetes"&gt;setup kubernetes&lt;/h1&gt;
&lt;p&gt;Reference : &lt;a href="https://phoenixnap.com/kb/install-kubernetes-on-ubuntu"&gt;https://phoenixnap.com/kb/install-kubernetes-on-ubuntu&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.15.3
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
kube-system coredns-5c98db65d4-r468f 0/1 Pending 0 2m3s &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
kube-system coredns-5c98db65d4-wcm2n 0/1 Pending 0 2m3s &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
kube-system etcd-mini1 1/1 Running 0 79s 192.168.1.100 mini1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
kube-system kube-apiserver-mini1 1/1 Running 0 76s 192.168.1.100 mini1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
kube-system kube-controller-manager-mini1 1/1 Running 0 72s 192.168.1.100 mini1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
kube-system kube-proxy-rzpkc 1/1 Running 0 2m4s 192.168.1.100 mini1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
kube-system kube-scheduler-mini1 1/1 Running 0 82s 192.168.1.100 mini1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id="install-calico"&gt;Install Calico&lt;/h2&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cychong@mini1:~$ wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml
--2019-09-08 21:53:13-- https://docs.projectcalico.org/v3.8/manifests/calico.yaml
Resolving docs.projectcalico.org (docs.projectcalico.org)... 178.128.115.5, 2400:6180:0:d1::575:a001
Connecting to docs.projectcalico.org (docs.projectcalico.org)|178.128.115.5|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 20628 (20K) [application/x-yaml]
Saving to: ‘calico.yaml’

calico.yaml 100%[====================================================================================&amp;gt;] 20.14K --.-KB/s in 0.08s

2019-09-08 21:53:14 (240 KB/s) - ‘calico.yaml’ saved [20628/20628]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Change &lt;code&gt;CALICO_IPV4POOL_CIDR&lt;/code&gt;&lt;/p&gt;</description></item><item><title>Ghost Season 5 - Helm</title><link>https://cychong47.github.io/post/2019/ghost-season-5-helm/</link><pubDate>Sun, 22 Sep 2019 13:14:16 +0900</pubDate><guid>https://cychong47.github.io/post/2019/ghost-season-5-helm/</guid><description>&lt;p&gt;ghost를 설치한 지 몇 년이 지났는데 그 동안 여러 가지 방법으로 &lt;a href="http://sosa0sa.com:2368/ghost-deployment-season-4/"&gt;Ghost 운용 환경을 구축&lt;/a&gt;해왔다.&lt;/p&gt;
&lt;p&gt;Host 환경, Docker, Ansible, kubernetes 에 이어 이번은 5번째 시즌인데 &lt;a href="https://helm.sh"&gt;Helm Chart&lt;/a&gt; 를 시용해서 설치해 보는 것이다. 처음 시작은 helm repository에 있는 공식(?) 공개된 helm chart를 이용하여 values.yaml 파일만 내 환경에 맞게 변경해서 사용하려던 것이었는데 아쉽게 아직은 그렇게 하기 힘든 것으로 보여 직접 helm chart를 만들어서 사용하고 있다. 이 문서는 그 과정을 기술한 것으로 향후 공식 helm chart를 활용할 수 있는 때가 오면 시즌 6에 해당하는 글을 또 쓸 듯 하다.&lt;/p&gt;</description></item><item><title>Calico CNI (draft)</title><link>https://cychong47.github.io/post/2019/calico-cni-1/</link><pubDate>Sun, 18 Aug 2019 15:20:01 +0900</pubDate><guid>https://cychong47.github.io/post/2019/calico-cni-1/</guid><description>&lt;h2 id="getting-started-with-calico-on-kubernetes"&gt;&lt;a href="https://www.dasblinkenlichten.com/getting-started-with-calico-on-kubernetes/"&gt;Getting started with Calico on Kubernetes&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Calico를 사용하는 경우 kubelet의 실행 옵션 중 &lt;code&gt;--network-plugin=cni&lt;/code&gt;와 같이 변경된다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;kube-controller-manager의 실행 옵션 중 &lt;code&gt;--allocate-node-cidrs=false&lt;/code&gt; 로 역시 변경된다. 이는 CNI(여기서는 Calico의 IPAM)에서 IP 주소를 할당하기 때문&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pod 내 route table에서는 host의 link local address를 default route로 사용한다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pod가 갖는 eth0 interface는 root(혹은 default) namespace에 존재하는 &amp;lsquo;cali&amp;rsquo;로 시작하는 interface와 veh pair 관계를 갖는다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;veth pairs는 아래 설명과 같이 서로 연결된 두 개의 interface를 의미하는데 한쪽으로 들어가면 연결된 다른 인터페이스로 나온다. 즉 pod의 eth0 interface를 통해 패킷을 전송하면 host의 cali interface로 나와 커널의 라우팅 혹은 iptable 처리를 받는다.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.fir3net.com/Networking/Terms-and-Concepts/virtual-networking-devices-tun-tap-and-veth-pairs-explained.html"&gt;https://www.fir3net.com/Networking/Terms-and-Concepts/virtual-networking-devices-tun-tap-and-veth-pairs-explained.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;What goes in one end will come out the other.&lt;/p&gt;</description></item><item><title>Setup Ghost in microk8s</title><link>https://cychong47.github.io/post/2019/setup-ghost-in-microk8s-2/</link><pubDate>Mon, 20 May 2019 16:00:56 +0900</pubDate><guid>https://cychong47.github.io/post/2019/setup-ghost-in-microk8s-2/</guid><description>&lt;h1 id="install-microk8s"&gt;Install microk8s&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://microk8s.io/"&gt;MicroK8s - Fast, Light, Upstream Developer Kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;설치는 위 링크에 있는 공식 홈페이지에 있는 대로 &lt;code&gt;snap&lt;/code&gt; 명령어 하나로 간단하게 설치할 수 있다.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cychong@mini1:~$ sudo snap install microk8s --classic
2019-05-18T09:43:53+09:00 INFO Waiting for restart...
microk8s v1.14.1 from Canonical✓ installed
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;설치된 &lt;code&gt;microk8s&lt;/code&gt;의 정보를 확인하려면 &lt;code&gt;snap info microk8s&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cychong@mini1:~$ sudo snap info microk8s
name: microk8s
summary: Kubernetes for workstations and appliances
publisher: Canonical✓
contact: &amp;lt;https://github.com/ubuntu/microk8s&amp;gt;
license: unset
description: |
 MicroK8s is a small, fast, secure, single node Kubernetes that installs on just about any Linux
 box. Use it for offline development, prototyping, testing, or use it on a VM as a small, cheap,
 reliable k8s for CI/CD. It's also a great k8s for appliances - develop your IoT apps for k8s and
 deploy them to MicroK8s on your boxes.
commands:
 - microk8s.config
 - microk8s.ctr
 - microk8s.disable
 - microk8s.enable
 - microk8s.inspect
 - microk8s.istioctl
 - microk8s.kubectl
 - microk8s.reset
 - microk8s.start
 - microk8s.status
 - microk8s.stop
services:
 microk8s.daemon-apiserver: simple, enabled, active
 microk8s.daemon-apiserver-kicker: simple, enabled, active
 microk8s.daemon-containerd: simple, enabled, active
 microk8s.daemon-controller-manager: simple, enabled, active
 microk8s.daemon-etcd: simple, enabled, active
 microk8s.daemon-kubelet: simple, enabled, active
 microk8s.daemon-proxy: simple, enabled, active
 microk8s.daemon-scheduler: simple, enabled, active
snap-id: EaXqgt1lyCaxKaQCU349mlodBkDCXRcg
tracking: stable
refresh-date: today at 09:44 KST
channels:
 stable: v1.14.1 2019-04-18 (522) 214MB classic
 candidate: v1.14.1 2019-04-15 (522) 214MB classic
 beta: v1.14.1 2019-04-15 (522) 214MB classic
 edge: v1.14.2 2019-05-17 (604) 217MB classic
 1.15/stable: –
 1.15/candidate: –
 1.15/beta: –
 1.15/edge: v1.15.0-alpha.3 2019-05-08 (578) 215MB classic
 1.14/stable: v1.14.1 2019-04-18 (521) 214MB classic
 1.14/candidate: v1.14.1 2019-04-15 (521) 214MB classic
 1.14/beta: v1.14.1 2019-04-15 (521) 214MB classic
 1.14/edge: v1.14.2 2019-05-17 (603) 217MB classic
 1.13/stable: v1.13.5 2019-04-22 (526) 237MB classic
 1.13/candidate: v1.13.6 2019-05-09 (581) 237MB classic
 1.13/beta: v1.13.6 2019-05-09 (581) 237MB classic
 1.13/edge: v1.13.6 2019-05-08 (581) 237MB classic
 1.12/stable: v1.12.8 2019-05-02 (547) 259MB classic
 1.12/candidate: v1.12.8 2019-05-01 (547) 259MB classic
 1.12/beta: v1.12.8 2019-05-01 (547) 259MB classic
 1.12/edge: v1.12.8 2019-04-24 (547) 259MB classic
 1.11/stable: v1.11.10 2019-05-10 (557) 258MB classic
 1.11/candidate: v1.11.10 2019-05-02 (557) 258MB classic
 1.11/beta: v1.11.10 2019-05-02 (557) 258MB classic
 1.11/edge: v1.11.10 2019-05-01 (557) 258MB classic
 1.10/stable: v1.10.13 2019-04-22 (546) 222MB classic
 1.10/candidate: v1.10.13 2019-04-22 (546) 222MB classic
 1.10/beta: v1.10.13 2019-04-22 (546) 222MB classic
 1.10/edge: v1.10.13 2019-04-22 (546) 222MB classic
installed: v1.14.1 (522) 214MB classic
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="enable-servicesmicrok8s"&gt;Enable services(microk8s)&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;cychong@mini1:~$ sudo microk8s.enable dashboard registry dns
Enabling dashboard
secret/kubernetes-dashboard-certs created
serviceaccount/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/kubernetes-dashboard created
service/monitoring-grafana created
service/monitoring-influxdb created
service/heapster created
deployment.extensions/monitoring-influxdb-grafana-v4 created
serviceaccount/heapster created
configmap/heapster-config created
configmap/eventer-config created
deployment.extensions/heapster-v1.5.2 created
dashboard enabled
Enabling the private registry
Enabling default storage class
deployment.extensions/hostpath-provisioner created
storageclass.storage.k8s.io/microk8s-hostpath created
Storage will be available soon
Applying registry manifest
namespace/container-registry created
persistentvolumeclaim/registry-claim created
deployment.extensions/registry created
service/registry created
The registry is enabled
Enabling DNS
Applying manifest
service/kube-dns created
serviceaccount/kube-dns created
configmap/kube-dns created
deployment.extensions/kube-dns created
Restarting kubelet
DNS is enabled
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;서비스 상태는 &lt;code&gt;microk8s.status&lt;/code&gt;로 확인 가능&lt;/p&gt;</description></item><item><title>ghost deployment season 4</title><link>https://cychong47.github.io/post/2019/ghost-deployment-season-4/</link><pubDate>Sun, 19 May 2019 12:45:58 +0900</pubDate><guid>https://cychong47.github.io/post/2019/ghost-deployment-season-4/</guid><description>&lt;p&gt;ghost blog를 구성해서 사용한 게 벌써 2014년 이다. 당시 0.x 버전 이었던 초반에는 얼마 못 가고 사라지지 않을까 걱정했는데 한참을 1.0버전을 발표하지 않더니 벌써 2.x 버전이다.&lt;/p&gt;
&lt;p&gt;그동안 내가 ghost 블로그를 운용하는 방식도 몇 번의 변화를 가졌다.&lt;/p&gt;
&lt;h3 id="시즌-1---brew--tar-ball"&gt;시즌 1 - brew &amp;amp; tar-ball&lt;/h3&gt;
&lt;p&gt;처음에는 매뉴얼 대로 직접 Node.js와 ghost 소스를 이용해서 직접 OS X에 설치해서 운용했다.&lt;/p&gt;
&lt;h3 id="시즌-2---docker"&gt;시즌 2 - Docker&lt;/h3&gt;
&lt;p&gt;그러다 Node.js 버전이 꼬이는 것도 그렇고, docker를 쓰면 ghost 버전이 새로 나왔을 때 편할 듯 해서 &lt;a href="http://sosa0sa.com:2368/move-to-docker/"&gt;docker를 쓰는 방식으로 변경했다&lt;/a&gt;. 이 시점에 docker의 stateless 속성을 이용하고, 데이터의 백업도 고려해서 ghost content는 Dropbox에 두고, docker 실행할 때 volume으로 마운트 하는 방식을 사용했다.
그 당시 ghost보다 먼저 운용하고 있던 &lt;a href="http://sosa0sa.com:2368/install-wordpress-with-docker/"&gt;wordpress도 함께 docker로 실행 환경을 바꿨다&lt;/a&gt;. wordpess는 ghost와 달리 MySql을 필요로 해서 docker-compose를 이용해서 두 개의 container를 연동해서 실행했다.&lt;/p&gt;</description></item><item><title>Kubernetes Networks</title><link>https://cychong47.github.io/post/2019/kubernetes-networks/</link><pubDate>Thu, 16 May 2019 14:07:53 +0900</pubDate><guid>https://cychong47.github.io/post/2019/kubernetes-networks/</guid><description>&lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1qCOlor16Wp5mHd6MQxB5gUEQILnijyDLIExEpqmee2k/edit?fbclid=IwAR0tlnpZ694c674Tmri3N3vgpaq4jH4zzPSA-RgFz1o4C49NgurHCezPDGo#gid=0"&gt;Kubernetes Networks in google docs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;이거 보면 IPv6를 사용해야 하는 경우 선택할 수 있는 CNI는 Calico, Cillium, Contiv, Tungsten Fabric 정도로 좁혀지네&lt;/p&gt;</description></item><item><title>(Summary) AT&amp;T Container Strategy</title><link>https://cychong47.github.io/post/2017/summary-at-t-container-strategy/</link><pubDate>Thu, 21 Sep 2017 15:21:45 +0900</pubDate><guid>https://cychong47.github.io/post/2017/summary-at-t-container-strategy/</guid><description>&lt;p&gt;From &lt;a href="https://www.youtube.com/watch?v=rYRiH3HZFN4&amp;amp;t=3s"&gt;https://www.youtube.com/watch?v=rYRiH3HZFN4&amp;amp;t=3s&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Presented in OpenStack Summit 2017 Boston&lt;/p&gt;
&lt;h2 id="container-will-be-used-for-workload-processing-after-2019"&gt;Container will be used for workload processing after 2019&lt;/h2&gt;
&lt;p&gt;&lt;img src="https://cychong47.github.io/images/2017/09/Openstack-Summit-2017---ATT-Container-Strategy-03-1.png" alt="Openstack-Summit-2017&amp;mdash;ATT-Container-Strategy-03-1"&gt;&lt;/p&gt;
&lt;p&gt;VNF is differ from Enterprise IT worklaod(4:19)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VNF is not a simple VM&lt;/li&gt;
&lt;li&gt;Maintain a state&lt;/li&gt;
&lt;li&gt;Complex network configuration&lt;/li&gt;
&lt;li&gt;Sophisticated Storage Connectivity&lt;/li&gt;
&lt;li&gt;HA is important&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2018-2019 vendor and open source project especially openstack should do something to meet the requirements.&lt;/p&gt;
&lt;p&gt;Even it takes some time for container to replace VM for workload perspective, running the Openstack service as a container is possible today.&lt;/p&gt;</description></item></channel></rss>